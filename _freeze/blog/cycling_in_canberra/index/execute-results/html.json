{
  "hash": "077d3f46da2cef0506f420ed2a7fdc4c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Cycling in Canberra ðŸš´\"\ndescription: \"Initial exploration into Canberra's cycling habits\"\nauthor: \"Johann Wagner\"\ndate: \"2025-03-18\"\ncategories:\n  - \"Exploratory Data Analysis\"\n  - \"Cycling\"\n  - \"Canberra\"\n---\n\n\n\n## Context\n### Personal Interest\nAs an avid cyclist commuting to work to work as a data analyst, combining the two seems natural. This analysis is a personal exploration into the Canberra's cycling habits, hopefully uncovering some patterns along the way.\n\n### Data Context\nThis dataset contains the number of trips counted by the bike barometer located on the Sullivan's Creek shared path at the intersection with MacArthur Avenue in O'Connor. Colloquially known as *Dickson Bike Path*, it is commonly used by commuters into Civic, university students heading to Campus in Acton, school students, and avid cyclists.\n\nThe data is recorded in hourly intervals, and is collected using an induction loop similar to systems used for traffic light monitoring. Trips in both directions are included in the recorded counts.\n\n## Setup and Configuration\n\nLet's begin by loading the neccessary R libraries.\n\n\n\n\n\n\n## Data Loading (with API)\nAccessing the Canberra Bike Barometer data requires interacting with an API. However, the API's default limit of 1000 rows necessitates a more strategic approach.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_url <- \"https://www.data.act.gov.au/resource/62sb-92ea.json\"\nlimit <- 1000\n\nget_data <- function(offset) {\n  url <- paste0(base_url, \"?$offset=\", offset, \"&$limit=\", limit)\n  request(url) |> \n    req_perform() |> \n    resp_body_json()\n}\n```\n:::\n\n\n\nTo overcome this limitation, we can use a `while` loop that fetches data in chunks of 1000 rows. The `tryCatch` function handles potential API errors, ensuring a robust data retrieval process. So long the http response code is 200[^1], the loop will not break. If the response is empty `length(result) == 0`, it means there's no more data and the loop will break.\n\n[^1]: When you see a 200 response, it signals that the data was retrieved without errors. The server has successfully processed our requests.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_data <- list()\noffset <- 0\n\nwhile (TRUE) {\n  print(paste(\"Offset:\", offset))\n  result <- tryCatch({\n    get_data(offset)\n  }, error = function(e) {\n    print(paste(\"Error fetching data:\", e$message))\n    return(NULL)\n  })\n  \n  if (is.null(result) || length(result) == 0) {\n    break\n  }\n  \n  bike_data <- c(bike_data, result)\n  offset <- offset + limit\n  \n  # Mindful of rate limits to avoid being blocked by the API\n  Sys.sleep(1)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Offset: 0\"\n[1] \"Offset: 1000\"\n[1] \"Offset: 2000\"\n[1] \"Offset: 3000\"\n[1] \"Offset: 4000\"\n[1] \"Offset: 5000\"\n[1] \"Offset: 6000\"\n[1] \"Offset: 7000\"\n[1] \"Offset: 8000\"\n[1] \"Offset: 9000\"\n[1] \"Offset: 10000\"\n[1] \"Offset: 11000\"\n[1] \"Offset: 12000\"\n[1] \"Offset: 13000\"\n[1] \"Offset: 14000\"\n[1] \"Offset: 15000\"\n[1] \"Offset: 16000\"\n[1] \"Offset: 17000\"\n[1] \"Offset: 18000\"\n[1] \"Offset: 19000\"\n[1] \"Offset: 20000\"\n[1] \"Offset: 21000\"\n[1] \"Offset: 22000\"\n[1] \"Offset: 23000\"\n[1] \"Offset: 24000\"\n[1] \"Offset: 25000\"\n[1] \"Offset: 26000\"\n[1] \"Offset: 27000\"\n[1] \"Offset: 28000\"\n[1] \"Offset: 29000\"\n[1] \"Offset: 30000\"\n[1] \"Offset: 31000\"\n[1] \"Offset: 32000\"\n[1] \"Offset: 33000\"\n[1] \"Offset: 34000\"\n[1] \"Offset: 35000\"\n[1] \"Offset: 36000\"\n[1] \"Offset: 37000\"\n[1] \"Offset: 38000\"\n[1] \"Offset: 39000\"\n[1] \"Offset: 40000\"\n[1] \"Offset: 41000\"\n[1] \"Offset: 42000\"\n[1] \"Offset: 43000\"\n[1] \"Offset: 44000\"\n[1] \"Offset: 45000\"\n[1] \"Offset: 46000\"\n[1] \"Offset: 47000\"\n```\n\n\n:::\n\n```{.r .cell-code}\nraw_bike_data <- bike_data |> \n  map_dfr(as_tibble)\n```\n:::\n\n\n\n### Sanity Checks - Raw Data\nHurray! Let's do some sanity checks to assess the data quality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(raw_bike_data)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |raw_bike_data |\n|Number of rows           |46200         |\n|Number of columns        |4             |\n|_______________________  |              |\n|Column type frequency:   |              |\n|character                |4             |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: character**\n\n|skim_variable                     | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:---------------------------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|date_time                         |         0|             1|  23|  23|     0|    46200|          0|\n|count                             |         0|             1|   1|   3|     0|      463|          0|\n|macarthur_ave_display_cyclist_in  |         0|             1|   1|   3|     0|      407|          0|\n|macarthur_ave_display_cyclist_out |         0|             1|   1|   3|     0|      319|          0|\n\n\n:::\n:::\n\n\n\nGreat! It's promising to see 46200 unique `datetime` values. We'll need to do some data cleaning and adjust `count`, `macarthur_ave_display_cyclist_in`, `macarthur_ave_display_cyclist_out` to be numeric values. Nevertheless, we can see that the min and max values are presumably between 0 and at most 999. The observed range seems plausible, and the absence of missing values is reassuring.\n\nThe great thing about the above data retrieval code is that hopefully it remains reproducible once ACT Government releases more recent data! This could be converted into an R package (a later project).\n\n## Data Cleaning\nLet's make those minor variable changes. We can now manipulate the data by converting the datetime column to a proper date-time format and ensure numeric columns are actually numeric types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_bike_data <- raw_bike_data |> \n  mutate(\n    date_time = date_time |> \n      str_sub(1, 13) |> \n      as_datetime(format = \"%Y-%m-%dT%H\"),\n    across(\n      !date_time,\n      as.integer\n    )\n  ) |> \n  # Avoid confusion with the count function\n  rename(cyclist_count = count)\nclean_bike_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 46,200 Ã— 4\n   date_time           cyclist_count macarthur_ave_display_cyclist_in\n   <dttm>                      <int>                            <int>\n 1 2017-11-22 00:00:00            13                                3\n 2 2017-11-22 01:00:00             2                                2\n 3 2017-11-22 02:00:00             0                                0\n 4 2017-11-22 03:00:00             0                                0\n 5 2017-11-22 04:00:00             1                                1\n 6 2017-11-22 05:00:00             9                                5\n 7 2017-11-22 06:00:00            28                               25\n 8 2017-11-22 07:00:00           157                              149\n 9 2017-11-22 08:00:00           358                              315\n10 2017-11-22 09:00:00           144                              131\n# â„¹ 46,190 more rows\n# â„¹ 1 more variable: macarthur_ave_display_cyclist_out <int>\n```\n\n\n:::\n:::\n\n\n\n### Sanity Checks - Clean Data\nNow, that the data is cleaned we can do some more sanity checks. The average number of cyclists passing through the Macarthur Avenue crossing is 43.5533766. However, there is quite a right-skewed distribution (mostly low counts by the hour). Interestingly, there's a slightly more cyclists heading into the city than out of the city. Our dataset ranges between 2017-11-22 and 2017-11-22.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(clean_bike_data)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |                |\n|:------------------------|:---------------|\n|Name                     |clean_bike_data |\n|Number of rows           |46200           |\n|Number of columns        |4               |\n|_______________________  |                |\n|Column type frequency:   |                |\n|numeric                  |3               |\n|POSIXct                  |1               |\n|________________________ |                |\n|Group variables          |None            |\n\n\n**Variable type: numeric**\n\n|skim_variable                     | n_missing| complete_rate|  mean|    sd| p0| p25| p50| p75| p100|hist  |\n|:---------------------------------|---------:|-------------:|-----:|-----:|--:|---:|---:|---:|----:|:-----|\n|cyclist_count                     |         0|             1| 43.55| 61.97|  0|   3|  22|  57|  549|â–‡â–â–â–â– |\n|macarthur_ave_display_cyclist_in  |         0|             1| 22.36| 44.18|  0|   1|   8|  24|  471|â–‡â–â–â–â– |\n|macarthur_ave_display_cyclist_out |         0|             1| 21.19| 36.24|  0|   1|   9|  25|  368|â–‡â–â–â–â– |\n\n\n**Variable type: POSIXct**\n\n|skim_variable | n_missing| complete_rate|min        |max                 |median              | n_unique|\n|:-------------|---------:|-------------:|:----------|:-------------------|:-------------------|--------:|\n|date_time     |         0|             1|2017-11-22 |2023-02-28 23:00:00 |2020-07-11 11:30:00 |    46200|\n\n\n:::\n:::\n\n\n\n## Initial visualisation explorations\nLet's visualise the hourly cyclist counts over time to identify any discernible trends.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_bike_data |> \n  ggplot(aes(\n    x = date_time,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nWow, what an ugly plot! It seems like the hourly data has some significant fluctuations (likely nighttime lows). Let's create a histogram to illustrate the right-skewed distribution of the `cyclist_count` variable and then aggregate to daily data and plot again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_bike_data |> \n  ggplot(aes(\n    x = cyclist_count\n  )) +\n  geom_histogram(\n    bins = 60\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe plot below shows the daily cyclist count. Again it's still quite confusing with a wide range of values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_cyclist_count <- clean_bike_data |> \n  mutate(\n    date = as_date(date_time)\n  ) |> \n  group_by(date) |> \n  summarise(\n    across(\n      where(is.numeric),\n      sum\n    ),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(\n    x = date,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  labs(\n    title = \"Cyclist Count by Date\",\n    x = \"Date\",\n    y = \"Cyclist Count\",\n  ) +\n  theme_minimal()\ndaily_cyclist_count\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Canberrans cycle more during the week\nLet's take the same plot and add some colour indicating the day of the week. Now this paints a better picture, we've discovered that Canberrans are cycling less over the weekend. This also brings out the seasonality in the dataset (perhaps Summer and Spring). But we also can see the impacts of the COVID-19 lockdowns and potentially the aftermath of working from home policies.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_cyclist_count_coloured <- daily_cyclist_count +\n  aes(colour = wday(date, label = TRUE)) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Day of Week\") +\n  theme(legend.position = \"bottom\")\ndaily_cyclist_count_coloured\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Morning and Afternoon Commutes\nWe've focused on the days of the week, let's now visualise the time of day and also facet by day of the week.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncyclist_count_by_hour <- clean_bike_data |> \n  mutate(\n    hour = hour(date_time)\n  ) |> \n  group_by(hour) |> \n  summarise(\n    across(\n      where(is.numeric),\n      mean\n    ),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(\n    x = hour,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Average Cyclist Count by Hour\",\n    x = \"Hour of the Day (24 hour time)\",\n    y = \"Cyclist Count\",\n  ) +\n  theme_minimal()\ncyclist_count_by_hour\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nMorning and afternoon work commutes! It also seems there's an increase around 3pm, maybe the finishing of school!\n\nLet's facet by day of the week to compare the hours by their respective days.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_bike_data_hourly_by_day_of_the_week <- clean_bike_data |>\n  mutate(\n    hour = hour(date_time),\n    day_of_week = date_time |> \n      date() |> \n      wday(week_start = 1, label = TRUE)\n  ) |>\n  group_by(hour, day_of_week) |>\n  summarise(\n    across(\n      where(is.numeric),\n      mean\n    ),\n    .groups = \"drop\"\n  )\n\ncyclist_count_by_hour_with_facet <- clean_bike_data_hourly_by_day_of_the_week |> \n  ggplot(aes(\n    x = hour,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Average Cyclist Count by Hour, Faceted by Day of Week\",\n    x = \"Hour of the Day (24 hour time)\",\n    y = \"Cyclist Count\"\n  ) +\n  theme_minimal() +\n  facet_wrap(~day_of_week)\n\ncyclist_count_by_hour_with_facet\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nVery interesting, as expected from the previous plots, weekends have less cyclists. We can see the morning work commutes during the week; however, there seems to be less commuters on Mondays and Fridays.\n\n## Next Steps\nMoving forward, it would be interesting to expand the analysis and incorporate weather information and maybe major events like Spilt Milk or Groovin. I also want to look into the effect of Public Holidays and perhaps see if there's any late night cyclists coming home from Thursday, Friday, Saturday nights out from Civic.\n\nAnother prospective idea would be looking into air quality data.\n\nCould even look into doing some time series forecasting with the `tidymodels` or `fable` R packages.\n\nHappy Cycling! ðŸš´\n\n## Resources used:\n\n* [ACT Bike Barometer - MacArthur Avenue](https://www.data.act.gov.au/Transport/ACT-Bike-Barometer-MacArthur-Avenue/62sb-92ea/about_data)\n* [Great video by Melissa Van Bussel (ggnot2) explaining how APIs work in R](https://www.youtube.com/watch?v=8tilyqp4bZY)\n* [Raelene analysed the same dataset (2018 - 2020)](https://rpubs.com/Raelene/828364)\n* [Helped me figure out what offset and limit parameters do](https://support.socrata.com/hc/en-us/articles/202949268-How-to-query-more-than-1000-rows-of-a-dataset)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
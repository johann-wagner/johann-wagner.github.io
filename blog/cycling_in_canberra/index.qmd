---
title: "Cycling in Canberra ðŸš´"
description: "Initial exploration into Canberra's cycling habits"
author: "Johann Wagner"
date: "2025-03-18"
format:
  html:
    highlight: pygments
categories:
  - "Exploratory Data Analysis"
  - "Cycling"
  - "Canberra"
---

## Context
### Personal Interest
As an avid cyclist commuting to work to work as a data analyst, combining the two seems natural. This analysis is a personal exploration into the Canberra's cycling habits, hopefully uncovering some patterns along the way.

### Data Context
This dataset contains the number of trips counted by the bike barometer located on the Sullivan's Creek shared path at the intersection with MacArthur Avenue in O'Connor. Colloquially known as *Dickson Bike Path*, it is commonly used by commuters into Civic, university students heading to Campus in Acton, school students, and avid cyclists.

The data is recorded in hourly intervals, and is collected using an induction loop similar to systems used for traffic light monitoring. Trips in both directions are included in the recorded counts.

## Setup and Configuration

Let's begin by loading the neccessary R libraries.
```{r}
#| output: false
library(tidyverse)
library(httr2)
library(skimr)
```

## Data Loading (with API)
Accessing the Canberra Bike Barometer data requires interacting with an API. However, the API's default limit of 1000 rows necessitates a more strategic approach.

```{r}
base_url <- "https://www.data.act.gov.au/resource/62sb-92ea.json"
limit <- 1000

get_data <- function(offset) {
  url <- paste0(base_url, "?$offset=", offset, "&$limit=", limit)
  request(url) |> 
    req_perform() |> 
    resp_body_json()
}
```

To overcome this limitation, we can use a `while` loop that fetches data in chunks of 1000 rows. The `tryCatch` function handles potential API errors, ensuring a robust data retrieval process. So long the http response code is 200[^1], the loop will not break. If the response is empty `length(result) == 0`, it means there's no more data and the loop will break.

[^1]: When you see a 200 response, it signals that the data was retrieved without errors. The server has successfully processed our requests.

```{r}
bike_data <- list()
offset <- 0

while (TRUE) {
  print(paste("Offset:", offset))
  result <- tryCatch({
    get_data(offset)
  }, error = function(e) {
    print(paste("Error fetching data:", e$message))
    return(NULL)
  })
  
  if (is.null(result) || length(result) == 0) {
    break
  }
  
  bike_data <- c(bike_data, result)
  offset <- offset + limit
  
  # Mindful of rate limits to avoid being blocked by the API
  Sys.sleep(1)
}

raw_bike_data <- bike_data |> 
  map_dfr(as_tibble)
```

### Sanity Checks - Raw Data
Hurray! Let's do some sanity checks to assess the data quality.
```{r}
skim(raw_bike_data)
```

Great! It's promising to see 46200 unique `datetime` values. We'll need to do some data cleaning and adjust `count`, `macarthur_ave_display_cyclist_in`, `macarthur_ave_display_cyclist_out` to be numeric values. Nevertheless, we can see that the min and max values are presumably between 0 and at most 999. The observed range seems plausible, and the absence of missing values is reassuring.

The great thing about the above data retrieval code is that hopefully it remains reproducible once ACT Government releases more recent data! This could be converted into an R package (a later project).

## Data Cleaning
Let's make those minor variable changes. We can now manipulate the data by converting the datetime column to a proper date-time format and ensure numeric columns are actually numeric types.
```{r}
clean_bike_data <- raw_bike_data |> 
  mutate(
    date_time = date_time |> 
      str_sub(1, 13) |> 
      as_datetime(format = "%Y-%m-%dT%H"),
    across(
      !date_time,
      as.integer
    )
  ) |> 
  # Avoid confusion with the count function
  rename(cyclist_count = count)
clean_bike_data
```

### Sanity Checks - Clean Data
Now, that the data is cleaned we can do some more sanity checks. The average number of cyclists passing through the Macarthur Avenue crossing is `r mean(clean_bike_data$cyclist_count)`. However, there is quite a right-skewed distribution (mostly low counts by the hour). Interestingly, there's a slightly more cyclists heading into the city than out of the city. Our dataset ranges between `r min(clean_bike_data$date_time) |> ymd()` and `r min(clean_bike_data$date_time) |> ymd()`.

```{r}
skim(clean_bike_data)
```

## Initial visualisation explorations
Let's visualise the hourly cyclist counts over time to identify any discernible trends.
```{r}
clean_bike_data |> 
  ggplot(aes(
    x = date_time,
    y = cyclist_count
  )) +
  geom_point() +
  theme_minimal()
```

Wow, what an ugly plot! It seems like the hourly data has some significant fluctuations (likely nighttime lows). Let's create a histogram to illustrate the right-skewed distribution of the `cyclist_count` variable and then aggregate to daily data and plot again.

```{r}
clean_bike_data |> 
  ggplot(aes(
    x = cyclist_count
  )) +
  geom_histogram(
    bins = 60
  ) +
  theme_minimal()
```
The plot below shows the daily cyclist count. Again it's still quite confusing with a wide range of values.
```{r}
daily_cyclist_count <- clean_bike_data |> 
  mutate(
    date = as_date(date_time)
  ) |> 
  group_by(date) |> 
  summarise(
    across(
      where(is.numeric),
      sum
    ),
    .groups = "drop"
  ) |> 
  ggplot(aes(
    x = date,
    y = cyclist_count
  )) +
  geom_point() +
  labs(
    title = "Cyclist Count by Date",
    x = "Date",
    y = "Cyclist Count",
  ) +
  theme_minimal()
daily_cyclist_count
```

## Canberrans cycle more during the week
Let's take the same plot and add some colour indicating the day of the week. Now this paints a better picture, we've discovered that Canberrans are cycling less over the weekend. This also brings out the seasonality in the dataset (perhaps Summer and Spring). But we also can see the impacts of the COVID-19 lockdowns and potentially the aftermath of working from home policies.

```{r}
daily_cyclist_count_coloured <- daily_cyclist_count +
  aes(colour = wday(date, label = TRUE)) +
  scale_colour_viridis_d() +
  labs(colour = "Day of Week") +
  theme(legend.position = "bottom")
daily_cyclist_count_coloured
```

## Morning and Afternoon Commutes
We've focused on the days of the week, let's now visualise the time of day and also facet by day of the week.
```{r}
cyclist_count_by_hour <- clean_bike_data |> 
  mutate(
    hour = hour(date_time)
  ) |> 
  group_by(hour) |> 
  summarise(
    across(
      where(is.numeric),
      mean
    ),
    .groups = "drop"
  ) |> 
  ggplot(aes(
    x = hour,
    y = cyclist_count
  )) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Cyclist Count by Hour",
    x = "Hour of the Day (24 hour time)",
    y = "Cyclist Count",
  ) +
  theme_minimal()
cyclist_count_by_hour
```

Morning and afternoon work commutes! It also seems there's an increase around 3pm, maybe the finishing of school!

Let's facet by day of the week to compare the hours by their respective days.
```{r}
clean_bike_data_hourly_by_day_of_the_week <- clean_bike_data |>
  mutate(
    hour = hour(date_time),
    day_of_week = date_time |> 
      date() |> 
      wday(week_start = 1, label = TRUE)
  ) |>
  group_by(hour, day_of_week) |>
  summarise(
    across(
      where(is.numeric),
      mean
    ),
    .groups = "drop"
  )

cyclist_count_by_hour_with_facet <- clean_bike_data_hourly_by_day_of_the_week |> 
  ggplot(aes(
    x = hour,
    y = cyclist_count
  )) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Cyclist Count by Hour, Faceted by Day of Week",
    x = "Hour of the Day (24 hour time)",
    y = "Cyclist Count"
  ) +
  theme_minimal() +
  facet_wrap(~day_of_week)

cyclist_count_by_hour_with_facet
```

Very interesting, as expected from the previous plots, weekends have less cyclists. We can see the morning work commutes during the week; however, there seems to be less commuters on Mondays and Fridays.

## Next Steps
Moving forward, it would be interesting to expand the analysis and incorporate weather information and maybe major events like Spilt Milk or Groovin. I also want to look into the effect of Public Holidays and perhaps see if there's any late night cyclists coming home from Thursday, Friday, Saturday nights out from Civic.

Another prospective idea would be looking into air quality data.

Could even look into doing some time series forecasting with the `tidymodels` or `fable` R packages.

Happy Cycling! ðŸš´

## Resources used:

* [ACT Bike Barometer - MacArthur Avenue](https://www.data.act.gov.au/Transport/ACT-Bike-Barometer-MacArthur-Avenue/62sb-92ea/about_data)
* [Great video by Melissa Van Bussel (ggnot2) explaining how APIs work in R](https://www.youtube.com/watch?v=8tilyqp4bZY)
* [Raelene analysed the same dataset (2018 - 2020)](https://rpubs.com/Raelene/828364)
* [Helped me figure out what offset and limit parameters do](https://support.socrata.com/hc/en-us/articles/202949268-How-to-query-more-than-1000-rows-of-a-dataset)

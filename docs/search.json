[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of my completed, “active”, and archived (ideas that didn’t fruition) projects. This is a pseudo-checklist for my future self to remind myself of all the extra features / ideas that I had at the time."
  },
  {
    "objectID": "projects/index.html#invasive-species-rshiny-dashboard",
    "href": "projects/index.html#invasive-species-rshiny-dashboard",
    "title": "Projects",
    "section": "Invasive Species RShiny Dashboard",
    "text": "Invasive Species RShiny Dashboard\n\nPurpose:\nThis project is complete. The dashboard is currently hosted on shinyapps.io. Whilst the dashboard is in a production state, there are still many improvements I would like to make. The dashboard has been shared with the Atlas of Living Australia and promoted on their website.\n\n\nProject Scope:\nThis project was the final assignment for the Data Science for Biologists (DS4B) university course.\nThe project had the following brief: Your task is to:\n\nchoose a dataset or combination of datasets\ndevelop some questions/aims related to that dataset, including appropriate background research\nuse R to address those questions/aims\nwrite your findings into a report of up to 2000 words and 10 figures\nsummarise your findings in a poster which you will present in the week 12 workshop\n\nIdeally, you should choose a combination of dataset and questions/aims that are interesting to you, because the more interesting you find it, the better your work will be!\n\n\nLinks:\n\nRShiny Dashboard\nPoster\nGitHub Repository"
  },
  {
    "objectID": "projects/index.html#spotify-exploratory-data-analysis",
    "href": "projects/index.html#spotify-exploratory-data-analysis",
    "title": "Projects",
    "section": "Spotify Exploratory Data Analysis",
    "text": "Spotify Exploratory Data Analysis\n\nPurpose:\nExplore Johann’s personal Spotify data and discover patterns in his listening habits.\n\n\nProject Status:\n\nCurrently working on looking at the streaming history data.\nNext step is to look at the playlist data and await the entire streaming history data to be available (late Jan 2024).\nThe final step is to create an RShiny dashboard to visualise the data and create some basic descriptive statistics.\nIdeally, this RShiny will allow individuals to upload their own personal Spotify data and explore their own listening habits.\nAdditionally, I would like to recreate my own version of the Spotify Wrapped.\nNeed to explore the Spotify API and see how I can integrate it into the RShiny dashboard.\nLook into other people’s Spotify and R projects to see what I can learn from them.\n\n\n\nLinks:\n\nGitHub Repository.\nSpotify API."
  },
  {
    "objectID": "projects/index.html#leadership-compass",
    "href": "projects/index.html#leadership-compass",
    "title": "Projects",
    "section": "Leadership Compass",
    "text": "Leadership Compass\n\nPurpose:\nCreate an R script that creates a visualisation of the results from the leadership compass survey.\n\n\nProject Status:\nThis project is currently in progress. There are still major improvements to be made to the visualisation and the data workflow. An excel file containing the survey results is currently being used as the data source. The next step is to create a Google Form to collect the survey results and then use the Google Form API to pull the data into the R script. Potentially, this can further be all incoroprated into an RShiny App.\n\n\nLinks:\n\nGitHub Repository"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "CV",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\nDownload Johann’s CV."
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "",
    "text": "This is a series of exploratory data analysis (EDA) projects on my Spotify data. The data was downloaded from my Spotify account on July 23rd, 2023. The data is downloaded as a zip file containing several json files and saved on my personal google drive. The json files are then converted into tibbles for analysis using the jsonlite package.\nThis quarto document is the first of several EDA projects. This project focuses on my streaming history. I’m interested in exploring my listening habits across the time period of the data. I’m also interested in exploring my listening habits across the days of the week.\nThis process is documented in the following sections:\n\nSetup and Configuration: Loading packages and googledrive API access\nData Loading: How to download and load the data?\nData Tidying: Get a tidy dataset\nData Cleaning: Ensure variables are in correct formats\nData Exploration: Answer one question and come up with two extra ones\n\nLet’s start exploring!"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#introduction",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#introduction",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "",
    "text": "This is a series of exploratory data analysis (EDA) projects on my Spotify data. The data was downloaded from my Spotify account on July 23rd, 2023. The data is downloaded as a zip file containing several json files and saved on my personal google drive. The json files are then converted into tibbles for analysis using the jsonlite package.\nThis quarto document is the first of several EDA projects. This project focuses on my streaming history. I’m interested in exploring my listening habits across the time period of the data. I’m also interested in exploring my listening habits across the days of the week.\nThis process is documented in the following sections:\n\nSetup and Configuration: Loading packages and googledrive API access\nData Loading: How to download and load the data?\nData Tidying: Get a tidy dataset\nData Cleaning: Ensure variables are in correct formats\nData Exploration: Answer one question and come up with two extra ones\n\nLet’s start exploring!"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#setup-and-configuration",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#setup-and-configuration",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\nFirst, let’s load in the packages we’ll need for this project and authorise access to my google drive.\n\n### \"Tidyverse\"-oriented packages:\n\n# The tidyverse is a collection of R packages designed for data science.\n# All packages share a similar design philosophy, grammar, and data structures.\n# Tidyverse includes packages such as:\n# ggplot2, dplyr, tidyr, readr, purr, tibble, stringr, lubridate, and forcats.\n### https://www.tidyverse.org/\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'forcats' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# To easily create data visualisations with simple and consistent syntax and grammar.\n# https://ggplot2.tidyverse.org/index.html\nlibrary(ggplot2)\n\n# To allow interaction between files on Google Drive and R.\n# https://googledrive.tidyverse.org/\nlibrary(googledrive)\n\nWarning: package 'googledrive' was built under R version 4.4.2\n\n### Other Packages:\n# To easily create summary statistics to understand and explore data.\n# https://docs.ropensci.org/skimr/\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.2\n\n# A fast JSON parser and generator.\n### https://cran.r-project.org/web/packages/jsonlite/index.html\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n# To easily enable file referencing in project-oriented workflows.\n# https://here.r-lib.org/\nlibrary(here)\n\nhere() starts at C:/Users/Johan/Documents/GitHub/johann-wagner.github.io\n\n# To easily format and scale data in visualisations.\n# https://scales.r-lib.org/\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Google Drive Authentication --------------------------------------------------\n\n# To establish a connection between a Google Drive account and R.\ndrive_auth()\n\n! Using an auto-discovered, cached token.\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n  See gargle's \"Non-interactive auth\" vignette for more details:\n  &lt;https://gargle.r-lib.org/articles/non-interactive-auth.html&gt;\nℹ The googledrive package is using a cached token for\n  'johann.wagner@gmail.com'.\n\n# Example of how to download from Google Drive\n# drive_download(\n#   # Where to download file from\n#   \"https://drive.google.com/file/d/1Fjq1r6016H4isB2Cx2wg-Xm9zY7lHhYV/view?usp=drive_link\",\n# \n#   # Where to save it locally\n#   path = here(\"foldertest\", \"text2\")\n#   )"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-loading",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-loading",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Data Loading",
    "text": "Data Loading\nTo access the data, I need to download it from my google drive. The data is requested from Johann’s Spotify account and downloaded as a zip file containing several json files. There are several different json files; however, for this analysis I’m only interested in the Streaming History files.\nYou will only have access if Johann has given you read access to the email you authorised in 0-00_setup_and_configuration.R.\n\n# Only download raw data if it hasn't already been downloaded\nif(!dir.exists(here(\"raw_data\"))) {\n  dir.create(here(\"raw_data\"), showWarnings = FALSE)\n\n  # List contents of Spotify Analysis Folder\n  spotify_dribble &lt;- drive_ls(\"Spotify Analysis\")\n  \n  # Download raw data\n  map2(\n    spotify_dribble$id,\n    spotify_dribble$name,\n    ~ drive_download(\n      file = as_id(.x),\n      path = here(\"raw_data\", .y),\n      overwrite = TRUE\n    )\n  )\n}\n\n\n\n# Read in individual raw json as nested lists\n# JRAW = RAW JSON\n# RAW_JSON causes alphabetical ordering inconveniences in R environment.\nJRAW_STREAMING_HISTORY_0 &lt;- read_json(\n  path = here(\n    \"raw_data\",\n    \"StreamingHistory0.json\"\n  )\n)\n\nJRAW_STREAMING_HISTORY_1 &lt;- read_json(\n  path = here(\n    \"raw_data\",\n    \"StreamingHistory1.json\"\n  )\n)\n\nJRAW_STREAMING_HISTORY_2 &lt;- read_json(\n  path = here(\n    \"raw_data\",\n    \"StreamingHistory2.json\"\n  )\n)"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-tidying",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-tidying",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Data Tidying",
    "text": "Data Tidying\nThese json files are then converted into tibbles for analysis using the jsonlite package. The tibbles are then combined into one tibble, as they all have the same columns. I suspect the reason why there are different files is because of the size of the data.\n\nRAW_STREAMING_HISTORY_0 &lt;- JRAW_STREAMING_HISTORY_0 %&gt;% \n  bind_rows() %&gt;% \n  as_tibble()\n\nRAW_STREAMING_HISTORY_1 &lt;- JRAW_STREAMING_HISTORY_1 %&gt;% \n  bind_rows() |&gt; \n  as_tibble()\n\nRAW_STREAMING_HISTORY_2 &lt;- JRAW_STREAMING_HISTORY_2 %&gt;% \n  bind_rows() |&gt; \n  as_tibble()\n\n# Combine all streaming history tibbles into one tibble\nRAW_STREAMING_HISTORY &lt;- bind_rows(\n  RAW_STREAMING_HISTORY_0,\n  RAW_STREAMING_HISTORY_1,\n  RAW_STREAMING_HISTORY_2\n)"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-cleaning",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-cleaning",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nLet’s ensure the variables are in the correct format.\n\nCLEANED_STREAMING_HISTORY &lt;- RAW_STREAMING_HISTORY |&gt; \n  mutate(\n    # Convert ms to minutes\n    min_played = as.numeric(msPlayed / 60000),\n    \n    # Convert artistName to factor\n    artist_name = as.factor(artistName),\n    \n    track_name = as.character(trackName),\n    \n    # Convert endTime into lubridate datetime\n    streaming_datetime = as_date(endTime, format = \"%Y-%m-%d %H:%M\")\n  ) |&gt; \n  \n  # Remove unnecessary columns\n  select(\n    artist_name,\n    track_name,\n    streaming_datetime,\n    min_played\n  )"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-exploration",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#data-exploration",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Data Exploration",
    "text": "Data Exploration\nThis data exploration has two objectives: 1. To get a sense of the data and to see if there are any issues with the data. 2. To answer several questions that I have about my listening habits.\n\nSanity Checks\nThere are 23456 rows in the CLEANED_STREAMING_HISTORY tibble, which is the number of songs/podcast episodes that I have listened to between 2022-07-11 and 2023-07-11. Let’s use the function skim() from the skimr package to get a sense check of the data.\n\nCLEANED_STREAMING_HISTORY |&gt; \n  skim()\n\n\nData summary\n\n\nName\nCLEANED_STREAMING_HISTORY\n\n\nNumber of rows\n23456\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntrack_name\n0\n1\n1\n179\n0\n7782\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nstreaming_datetime\n0\n1\n2022-07-11\n2023-07-11\n2022-12-01\n337\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nartist_name\n0\n1\nFALSE\n4324\nPar: 1147, Van: 271, Cou: 217, Lof: 183\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmin_played\n0\n1\n3.01\n2.8\n0\n1.56\n3.08\n4\n82.34\n▇▁▁▁▁\n\n\n\n\n\nThere are 4 columns in the CLEANED_STREAMING_HISTORY tibble. There are 4324 unique artists and 7782 unique tracks in the CLEANED_STREAMING_HISTORY tibble. It is interesting that the shortest track_name has a length of 1 characters and the longest track_name has a length of 179 characters. Interestingly, the shortest track_name has a length of 1 characters. I wonder what song that is. The date ranges between 2022-07-11 and 2023-07-11.\nIt seems like the data mostly makes sense and that there are a wide range of song names and artist names.\n\n\nReshape Data: Streaming per day\nLet’s reshape the data so that we can see how much I have streamed per day.\n\nSTREAMING_HISTORY_PER_DAY &lt;- CLEANED_STREAMING_HISTORY |&gt; \n  group_by(streaming_datetime) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  )\nSTREAMING_HISTORY_PER_DAY\n\n# A tibble: 337 × 2\n   streaming_datetime total_hours_played\n   &lt;date&gt;                          &lt;dbl&gt;\n 1 2022-07-11                      2.41 \n 2 2022-07-12                      5.31 \n 3 2022-07-13                      2.78 \n 4 2022-07-14                      3.52 \n 5 2022-07-15                      4.38 \n 6 2022-07-16                      9.42 \n 7 2022-07-17                      4.99 \n 8 2022-07-18                      3.64 \n 9 2022-07-19                      3.78 \n10 2022-07-20                      0.147\n# ℹ 327 more rows\n\n\n\n\nWhat were the top 5 days I listened to music?\nLet’s now investigate what the top 5 days I listened to music were and include the day of the week.\n\nTOP_SONGS &lt;- STREAMING_HISTORY_PER_DAY |&gt;\n  mutate(\n    day_of_week = wday(streaming_datetime, label = TRUE)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  head(5)\nTOP_SONGS\n\n# A tibble: 5 × 3\n  streaming_datetime total_hours_played day_of_week\n  &lt;date&gt;                          &lt;dbl&gt; &lt;ord&gt;      \n1 2023-05-30                       17.2 Tue        \n2 2023-02-18                       13.9 Sat        \n3 2022-11-01                       11.9 Tue        \n4 2022-12-16                       11.8 Fri        \n5 2022-09-02                       11.2 Fri        \n\n\nIt seems like 2023-05-30 and 2023-02-18 were two days when I listened to a LOT of music.\nLet’s pull it back and look at the aggregate again; I wonder what the most listened to days are?\n\nSTREAMING_HISTORY_PER_DAY |&gt; \n  mutate(\n    day_of_week = wday(streaming_datetime, label = TRUE)\n    ) |&gt; \n  group_by(day_of_week) |&gt;\n  summarise(\n    total_hours_played = sum(total_hours_played)\n  ) |&gt;\n  arrange(desc(total_hours_played))\n\n# A tibble: 7 × 2\n  day_of_week total_hours_played\n  &lt;ord&gt;                    &lt;dbl&gt;\n1 Mon                       199.\n2 Tue                       185.\n3 Sat                       180.\n4 Fri                       164.\n5 Thu                       157.\n6 Sun                       151.\n7 Wed                       140.\n\n\nSurprisingly, it seems like Mondays are the days where I have listened to the most streamed music. I wonder if this is because I listen to music on my commute to work? Although, I don’t think I was really working consistently in 2022-23.\nSo potentially this is because I listen to music when I was studying? To answer this question and gain more insights, I would need to look at my calendar and see what I was doing on those days.\n\n\nHow did my streaming time vary by day?\nLet’s plot the total hours played per day.\n\nGGPLOT_HOURS_PLAYED_PER_DAY &lt;- STREAMING_HISTORY_PER_DAY |&gt; \n  ggplot(aes(x = streaming_datetime, y = total_hours_played)) +\n  \n  geom_point() +\n  geom_line() +\n  \n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Hours Played Per Day\",\n    subtitle = \"Spotify Streaming History\"\n  ) +\n  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(\n      size = 20,\n      face = \"bold\"\n    ),\n    plot.subtitle = element_text(\n      size = 15\n    ),\n    axis.title = element_text(\n      size = 15\n    ),\n    axis.text = element_text(\n      size = 10\n    )\n  )\n\nGGPLOT_HOURS_PLAYED_PER_DAY\n\n\n\n\n\n\n\n\nThere is a high fluctuation in the number of hours played per day with some days, when very little music was played and some days were a lot of music was played. It seems that there are two days in particular, where I have listened to a lot of music. Let’s investigate these days further, we know that the days are: 2023-05-30 and 2023-02-18. What did I do on these two days? Let’s also include a smoothed line.\n\nGGPLOT_HOURS_PLAYED_PER_DAY +\n  \n  geom_point(aes(\n    colour = ifelse(\n      streaming_datetime == as.Date(\"2023-05-30\") | \n        streaming_datetime == as.Date(\"2023-02-18\"),\n      \"red\",\n      \"darkgrey\"\n    )\n    )\n  ) +\n  geom_line(colour = \"darkgrey\") +\n  geom_smooth() +\n  geom_label(\n    label = \"Flying to Australia\",\n    x = as.Date(\"2023-05-30\"),\n    y = STREAMING_HISTORY_PER_DAY |&gt; \n      filter(streaming_datetime == as.Date(\"2023-05-30\")) |&gt; \n      pull(total_hours_played),\n    vjust = -0.5\n  ) +\n  geom_label(\n    label = \"Flying to Austria\",\n    x = as.Date(\"2023-02-18\"),\n    y = STREAMING_HISTORY_PER_DAY |&gt; \n      filter(streaming_datetime == as.Date(\"2023-02-18\")) |&gt; \n      pull(total_hours_played),\n    vjust = -0.5\n  ) +\n    expand_limits(\n    y = c(0, 20)\n  ) +\n  scale_color_identity()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFlying in the plane and listening to music! That makes sense. The smoothed line suggests that there was more music listened to in the second half of 2022 than the first half of 2023.\n\n\nHow did my streaming time vary by month?\nLet’s investigate this further: what was the total number of hours played per month?\n\nSTREAMING_HISTORY_PER_MONTH &lt;- CLEANED_STREAMING_HISTORY |&gt; \n  mutate(\n    month_floor = floor_date(streaming_datetime, unit = \"month\"),\n    year_floor = floor_date(streaming_datetime, unit = \"year\")\n  ) |&gt; \n  group_by(month_floor, year_floor) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  )\n\n`summarise()` has grouped output by 'month_floor'. You can override using the\n`.groups` argument.\n\n\nLet’s plot the total hours played per month.\n\nSTREAMING_HISTORY_PER_MONTH |&gt; \n  ggplot(aes(x = month_floor, y = total_hours_played)) +\n  \n  geom_point() +\n  geom_line() +\n  \n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Hours Played Per Month\",\n    subtitle = \"Spotify Streaming History\"\n  ) +\n  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(\n      size = 20,\n      face = \"bold\"\n    ),\n    plot.subtitle = element_text(\n      size = 15\n    ),\n    axis.title = element_text(\n      size = 15\n    ),\n    axis.text = element_text(\n      size = 10\n    )\n  )\n\n\n\n\n\n\n\n\nThere seems to be a bit of a pattern. Before I went backpacking (Jan 2023), I was listening to a lot more music. Let’s calculate the total number of hours played in both years and see how different they are.\n\nSTREAMING_HISTORY_PER_MONTH |&gt; \n  group_by(year_floor) |&gt;\n  summarise(\n    total_hours_played = sum(total_hours_played)\n  )\n\n# A tibble: 2 × 2\n  year_floor total_hours_played\n  &lt;date&gt;                  &lt;dbl&gt;\n1 2022-01-01               697.\n2 2023-01-01               481.\n\n\nThere definitely seems like there is a major difference between the two years. I wonder if this is because I was travelling in 2023 and therefore didn’t have as much time to listen to music. Let’s investigate this further.\n\n\nWho were my top artists?\nLet’s investigate who my top artists are. We will do this by grouping by artist name and then calculating the total number of hours played.\n\nCLEANED_STREAMING_HISTORY |&gt; \n  group_by(artist_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  head(10) |&gt; \n  ggplot(aes(x = reorder(artist_name, total_hours_played), y = total_hours_played)) +\n  geom_col(aes(fill = ifelse(total_hours_played &gt; 20, \"orange\", \"grey\"))) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = seq(0, 100, 10)\n  ) +\n  scale_fill_identity() +\n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Top Artists\",\n    subtitle = \"Spotify Streaming History: July 2022 - July 2023\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs expected, I’m a massive Parcels fan and the data shows it! Let’s look at my top artists for each month.\n\nCLEANED_STREAMING_HISTORY |&gt; \n  mutate(\n    month_floor = floor_date(streaming_datetime, unit = \"month\")\n  ) |&gt; \n  group_by(month_floor, artist_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  group_by(month_floor) |&gt; \n  slice(1) |&gt; \n  ggplot(aes(x = month_floor, y = total_hours_played, fill = artist_name)) +\n  geom_col() +\n  scale_fill_viridis_d() +\n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Top Artists Per Month\",\n    subtitle = \"Spotify Streaming History: July 2022 - July 2023\"\n  ) +\n  theme_minimal()\n\n`summarise()` has grouped output by 'month_floor'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nWow, Parcels really was my favourite artist consistently throughout the time range, although from April 2023 onwards, it seems I started listening to more podcasts. A further question for future investigation: How does my podcast listening behaviour change over time.\n\n\nWhat were my top songs?\nLet’s move onto top songs. We will do this by grouping by track name and then calculating the total number of hours played.\n\nCLEANED_STREAMING_HISTORY |&gt; \n  group_by(track_name, artist_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  head(10) |&gt; \n  ggplot(aes(x = reorder(track_name, total_hours_played), y = total_hours_played)) +\n  geom_col(aes(fill = ifelse(artist_name == \"Parcels\", \"orange\", \"grey\"))) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = seq(0, 10, 2)\n  ) +\n  scale_fill_identity() +\n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Top Songs\",\n    subtitle = \"Spotify Streaming History: July 2022 - July 2023\\nOrange = Parcels\"\n  ) +\n  theme_minimal()\n\n`summarise()` has grouped output by 'track_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nFive of the top 10 songs were songs from Parcels.\nLet’s look at the top songs for each month.\n\nCLEANED_STREAMING_HISTORY |&gt; \n  mutate(\n    month_floor = floor_date(streaming_datetime, unit = \"month\")\n  ) |&gt; \n  group_by(month_floor, track_name, artist_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  group_by(month_floor) |&gt; \n  slice(1) |&gt; \n  mutate(\n    fill_colour = case_when(\n      track_name == \"Lost in Music - Dimitri from Paris Remix\" ~ \"pink\",\n      artist_name == \"Parcels\" ~ \"orange\",\n      .default = \"grey\"\n      )\n  ) |&gt; \n  ggplot(aes(x = month_floor, y = total_hours_played, fill = fill_colour)) +\n  geom_col() +\n  scale_fill_identity() +\n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Top Songs Per Month\",\n    subtitle = \"Spotify Streaming History: July 2022 - July 2023\\nOrange = Parcels\\nPink = Lost in Music - Dimitri from Paris Remix\"\n  ) +\n  theme_minimal()\n\n`summarise()` has grouped output by 'month_floor', 'track_name'. You can\noverride using the `.groups` argument.\n\n\n\n\n\n\n\n\n\nIt seems that I listened to Lost in Music - Dimitri from Paris Remix a lot in July/August 2022. Parcels was my top artist for every month, but it seems that I listened to them a lot more in October 2022 and January/Febuary 2023.\n\n\nHow did my top 10 songs vary across time?\nLet’s investigate how my top 10 songs varied across time. We will do this by grouping by track name and then calculating the total number of hours played.\n\ntop_ten_songs &lt;- CLEANED_STREAMING_HISTORY |&gt; \n  group_by(track_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt; \n  arrange(desc(total_hours_played)) |&gt; \n  head(5) |&gt; \n  pull(track_name)\n\nCLEANED_STREAMING_HISTORY |&gt;\n  filter(track_name %in% top_ten_songs) |&gt; \n  mutate(\n    month_floor = floor_date(streaming_datetime, unit = \"month\")\n  ) |&gt;\n  group_by(month_floor, track_name) |&gt;\n  summarise(\n    total_hours_played = sum(min_played / 60)\n  ) |&gt;\n  ggplot(aes(x = month_floor, y = total_hours_played, colour = track_name)) +\n  \n  geom_point() +\n  geom_line() +\n  \n  labs(\n    x = \"\",\n    y = \"Hours Played\",\n    title = \"Top 5 Songs - Hours Played Per Day\",\n    subtitle = \"Spotify Streaming History\",\n    colour = \"Track Name\"\n  ) +\n  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(\n      size = 20,\n      face = \"bold\"\n    ),\n    plot.subtitle = element_text(\n      size = 15\n    ),\n    axis.title = element_text(\n      size = 15\n    ),\n    axis.text = element_text(\n      size = 10\n    )\n  )\n\n`summarise()` has grouped output by 'month_floor'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nThis is super interesting. It seems that there are some rough patterns in my top 5 songs. For example, “Lost in Music - Dimitri from Paris Remix” was played a lot in the first half of 2022 and then not at all in the first half of 2023. Similarly, “The Girl” has a similar downwards trend. “Tieduprightnow” was played a lot in the new year (2023); however, also dropped. “Free” and “Bitter Sweet Symphony” were almost perfectly positively correlated with each other with the exception of late 2022.\nI wonder if I could do this analysis for all of my songs and then create a grouping/cluster analysis to see if there are any temporal patterns in my music listening? Are there some songs that I listen to with other songs? Do these songs group together because I usually listen to them from the same playlist? Can I somehow link/predict my playlist data and my streaming data?"
  },
  {
    "objectID": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#moving-forward",
    "href": "blog/spotify_exploratory_data_analysis_streaming_history_data/index.html#moving-forward",
    "title": "Spotify Exploratory Data Analysis - Streaming History Data",
    "section": "Moving Forward",
    "text": "Moving Forward\nThere are quite a few questions that I would like to explore in the future. For example: - I would like to explore how my podcast listening behaviour change over time. - I would like to explore how my top 10 songs varied across time and utilise the gganimate package. - I would really like to do some time series analysis on my streaming history. - I’m curious on linking my streaming history data with my playlist data. I wonder if I can predict my playlist data based on my streaming history data. I think I would typically use Spotify by listening to my playlists, so potentially doing some clustering/grouping analysis on my streaming history data and then linking it to my playlist data would be interesting.\nThese are all questions that I would like to explore in future! But for now, these were some great first initial data explorations of my Spotify streaming history. I hope you enjoyed reading this post and I hope you learned something new about Spotify streaming history data analysis. If you have any questions or comments, please feel free to reach out to me. I would love to hear from you! :)"
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html",
    "href": "blog/invasive_species_dashboard_report/index.html",
    "title": "Invasive Animal Species Dashboard",
    "section": "",
    "text": "This is a university assignment for the course Data Science for Biology at the Australian National University (ANU). The purpose of the assignment was to formulate a research question, use R and a relevant dataset to address the question and communicate the analysis into a report and a poster.\nI developed an RShiny Dashboard visualising seven different invasive species by state/territory using ALA data through the galah package with a focus on spatial and temporal visualisations. The dashboard also allows users to download the cleaned dataset for the selected invasive animal species for a particular state/territory.\nThe Dashboard is available here.\nThe GitHub repository is available here.\nEnjoy! :)"
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#initial-aims",
    "href": "blog/invasive_species_dashboard_report/index.html#initial-aims",
    "title": "Invasive Animal Species Dashboard",
    "section": "Initial Aims:",
    "text": "Initial Aims:\n\nDevelop an RShiny app that showcases the spatial and temporal (monthly) occurrence of the top 5-10 7 invasive animal species in Australia by state/territory and by regionality (urban vs. regional)\nVisualise and calculate the proportion of the selected invasive animal species observed in an urban area\nCreate a download option for users to download the cleaned dataset for the selected invasive animal species for a particular state/territory"
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#justifications",
    "href": "blog/invasive_species_dashboard_report/index.html#justifications",
    "title": "Invasive Animal Species Dashboard",
    "section": "Justifications:",
    "text": "Justifications:\nInitially, I was interested in the spatial distribution of magpies in Canberra as I’m an avid cyclist and wanted to find the hotspots of magpie records and linking/statistically model this data with a collaborative magpie swooping database called magpiealert.com. Unfortunately, the data was not available and the sampling bias of the occurrence data may not be truly representative, as observations are more likely to be near roads, airports, and major urban areas (Kellie, 2022).\nThis made me interested in simply visualising the data spatially, instead of using any statistical methodologies and focusing the project on whether there would be major differences/patterns of occurrence records in urban vs. non-urban/regional areas. Unfortunately, given the scope of the project, calculating the urban vs. non-urban analysis would have been too much scope-creep, so this aim was ignored (maybe post-project exploration).\nHughes et al. (2021) and Guerin et al. (2016) highlight the spatial sampling bias associated with biodiversity data suggesting that these biases must first be understood before applying any statistical modelling methods. Interestingly, Australian research by Piccolo et al. (2020) show that reptile research locations are highly positively correlated with proximity to universities suggesting biodiversity research locations are closely associated by accessibility. Similarly, temporal biases can also influence results, as species distributions can change across time (Boakes et al., 2010).\nThe Atlas of Living Australia already have a browser-based spatial visualisation tool, where a specific species can be selected and plotted on a map. However, there seemed to not be a functionality to comparing both the spatial and temporal (by month) variables.\nThis dashboard is the initial step in data exploration step and in the essence of open and citizen-science, I wanted to make my cleaned data easily available through the dashboard. Data sharing enables for further analysis and reusability (Tedersoo et al., 2021, Ramachandran et al., 2021). I wanted my dashboard to follow the FAIR Data Principles (Wilkinson et al., 2016).\nI chose invasive animal species, because of the rabbits I see when cycling in Canberra. The other 6 species were chosen arbitrarily based off this article, which summarises the results from Kearney et al.’s paper (2018).\nA deeper understanding of my thought process is documented in GitHub Issue #4.\nThe analysis was based on this ALA Labs article (Smith et al., 2023)."
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#aim-1-dashboard-and-visualisations",
    "href": "blog/invasive_species_dashboard_report/index.html#aim-1-dashboard-and-visualisations",
    "title": "Invasive Animal Species Dashboard",
    "section": "Aim 1: Dashboard and Visualisations",
    "text": "Aim 1: Dashboard and Visualisations\n\nFolder System\nThe RShiny app can be found in the GitHub folder DS4B-final-project/invasive_species_dashboard. Similar to R Project environments, the RShiny app is all self-contained in this folder. The individual R scripts in the DS4B-final-project/scripts folder must be run first to produce and save the dashboard data within the RShiny folder for most recent data.\n\n\nThe RShiny App\nThe RShiny folder contains four files:\n\n0-00_setup_and_configuration.R: loads relevant packages and custom-made functions to create the analysis/visualisations/RShiny.\ndashboard_data.csv: is the cleaned occurrence data for the seven invasive species, including spatial and temporal variables.\nserver.R contains the back-end logic/code that takes the two inputs to dynamically create/adjust the visualisations/titles/downloads.\nui.R contains the front-end user-interface design and layout of the app.\n\n\n\nSpatial Visualisation: European Rabbits in ACT\nLet’s create the spatial visualisation using European Rabbits and ACT as the inputs.\n\nlibrary(scales)\nspecies_simple_name &lt;- \"European Rabbits\"\nstate_name &lt;- \"Australian Capital Territory\"\n\n\ndashboard_data &lt;- read_csv(\"processed_data/dashboard_data.csv\")\n\nspatial_data &lt;- dashboard_data %&gt;%\n  filter(\n    simpleName == species_simple_name,\n    state      == state_name\n  )\n\n\n\ncapital_cities_data &lt;- tibble::tribble(\n  ~state,                         ~city,       ~lat,     ~lon,\n  \"Australian Capital Territory\", \"Canberra\",  -35.2809, 149.1300,\n) %&gt;%\n  filter(state == state_name)\n\n\n\nstate2021 %&gt;%\n\n  filter(state_name_2021 == state_name) %&gt;%\n  ggplot() +\n\n  geom_sf(\n    aes(geometry = geometry),\n    fill = \"#E5E4E2\"\n  ) +\n  geom_point(\n    data = spatial_data,\n    aes(\n      x = decimalLongitude,\n      y = decimalLatitude,\n      size = simpleName\n    ),\n    alpha = 0.6,\n    colour = \"#1b9e77\"\n  ) +\n  geom_point(\n    data = capital_cities_data,\n    aes(\n      x = lon,\n      y = lat,\n      shape = city\n    ),\n    colour = \"#d95f02\",\n    size = 4\n  ) +\n  coord_sf() +\n\n  theme_minimal() +\n\n  # Ref [1]\n  labs(\n    title = bquote(\"There are\" ~\n                     bold(.({\n                       spatial_data %&gt;%\n                         nrow() %&gt;%\n                         comma()\n                     })) ~\n                     \"records:\"),\n    subtitle = bquote(bold(.(\n      species_simple_name\n    )) ~\n      \"in\" ~\n      bold(.(state_name))),\n    shape = \"Capital City\",\n    size = \"Invasive Species\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank(),\n\n    plot.title = element_text(size = 13),\n\n    # Ref [2]: Improve readability\n    legend.text = element_text(size = 12),\n    legend.title = element_text(\n      size = 12,\n      face = \"bold\"\n    ),\n    legend.direction = \"vertical\",\n    legend.background = element_rect(\n      fill = \"#E5E4E2\",\n      colour = \"#708090\"\n    ),\n    legend.position = \"right\",\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.box.just = \"right\",\n    legend.margin = margin(6, 6, 6, 6),\n\n    panel.background = element_rect(fill = \"transparent\", colour = \"transparent\")\n  )\n\n\n\n\n\n\n\n# References:\n# 1: https://labs.ala.org.au/posts/2023-05-16_dingoes/post.html\n# 2: https://r-graph-gallery.com/239-custom-layout-legend-ggplot2.html\n\nEach green point represents one record. We can see that the majority of records are near Canberra with some patterns in Namadgi National Park, like Boboyan Road. Let’s visualise some more maps.\n\n\nTemporal Visualisation\nThe aim of creating a circular monthly bar plot of record proportions was to explore potential temporal patterns. This plot suggests two things:\n\nThe time frequency when citizen scientists submit data and/or when professional/government teams measure/monitor invasive species. Maybe, annual monitoring occurs in May?\nThe actual temporal species distribution. For example, maybe there are more cats in May in SA, because of reproduction behaviour?\n\nLet’s create it for Feral Cats in South Australia (A screenshot was used, see GitHub for code - this blog post is already so long…).\n\n\n\nTemporal Feral Cats in SA"
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#aim-2-regionality",
    "href": "blog/invasive_species_dashboard_report/index.html#aim-2-regionality",
    "title": "Invasive Animal Species Dashboard",
    "section": "Aim 2: Regionality",
    "text": "Aim 2: Regionality\nThis aim was completely abandoned, because of the scope of the project was getting too big. However, adding the capital city point highlights how majority of the selected species cluster around the capital city. Additionally, a zoom function was added to the Shiny app for users to zoom into specific patterns and areas of interest."
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#aim-3-data-download",
    "href": "blog/invasive_species_dashboard_report/index.html#aim-3-data-download",
    "title": "Invasive Animal Species Dashboard",
    "section": "Aim 3: Data Download",
    "text": "Aim 3: Data Download\nEnsuring the data followed FAIR principles included making the data findable, such as referencing the DOIs on the dashboard. The user can select the two inputs they are interested in and can download the respective csv. The option to download the entire dashboard dataset is also available."
  },
  {
    "objectID": "blog/invasive_species_dashboard_report/index.html#github-use-final-rshiny-app",
    "href": "blog/invasive_species_dashboard_report/index.html#github-use-final-rshiny-app",
    "title": "Invasive Animal Species Dashboard",
    "section": "GitHub Use / Final RShiny App",
    "text": "GitHub Use / Final RShiny App\n\nFinal UI and visualisation design improvements were made using GitHub Issues to manage these efforts.\nGitHub branching was used for each Issue.\nGitHub Releases were used to distinguish major developments of the RShiny App.\n\nPlease see the README file for more information.\n\n\n\n\nInvasive Animal Species Dashboard (Version 1.0.0)"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html",
    "href": "blog/cycling_in_canberra/index.html",
    "title": "Cycling in Canberra 🚴",
    "section": "",
    "text": "As an avid cyclist commuting to work to work as a data analyst, combining the two seems natural. This analysis is a personal exploration into the Canberra’s cycling habits, hopefully uncovering some patterns along the way.\n\n\n\nThis dataset contains the number of trips counted by the bike barometer located on the Sullivan’s Creek shared path at the intersection with MacArthur Avenue in O’Connor. Colloquially known as Dickson Bike Path, it is commonly used by commuters into Civic, university students heading to Campus in Acton, school students, and avid cyclists.\nThe data is recorded in hourly intervals, and is collected using an induction loop similar to systems used for traffic light monitoring. Trips in both directions are included in the recorded counts."
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#context",
    "href": "blog/cycling_in_canberra/index.html#context",
    "title": "Cycling in Canberra 🚴",
    "section": "",
    "text": "As an avid cyclist commuting to work to work as a data analyst, combining the two seems natural. This analysis is a personal exploration into the Canberra’s cycling habits, hopefully uncovering some patterns along the way.\n\n\n\nThis dataset contains the number of trips counted by the bike barometer located on the Sullivan’s Creek shared path at the intersection with MacArthur Avenue in O’Connor. Colloquially known as Dickson Bike Path, it is commonly used by commuters into Civic, university students heading to Campus in Acton, school students, and avid cyclists.\nThe data is recorded in hourly intervals, and is collected using an induction loop similar to systems used for traffic light monitoring. Trips in both directions are included in the recorded counts."
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#setup-and-configuration",
    "href": "blog/cycling_in_canberra/index.html#setup-and-configuration",
    "title": "Cycling in Canberra 🚴",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\nLet’s begin by loading the neccessary R libraries.\n\nlibrary(tidyverse)\nlibrary(httr2)\nlibrary(skimr)"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#data-loading-with-api",
    "href": "blog/cycling_in_canberra/index.html#data-loading-with-api",
    "title": "Cycling in Canberra 🚴",
    "section": "Data Loading (with API)",
    "text": "Data Loading (with API)\nAccessing the Canberra Bike Barometer data requires interacting with an API. However, the API’s default limit of 1000 rows necessitates a more strategic approach.\n\nbase_url &lt;- \"https://www.data.act.gov.au/resource/62sb-92ea.json\"\nlimit &lt;- 1000\n\nget_data &lt;- function(offset) {\n  url &lt;- paste0(base_url, \"?$offset=\", offset, \"&$limit=\", limit)\n  request(url) |&gt; \n    req_perform() |&gt; \n    resp_body_json()\n}\n\nTo overcome this limitation, we can use a while loop that fetches data in chunks of 1000 rows. The tryCatch function handles potential API errors, ensuring a robust data retrieval process. So long the http response code is 2001, the loop will not break. If the response is empty length(result) == 0, it means there’s no more data and the loop will break.\n1 When you see a 200 response, it signals that the data was retrieved without errors. The server has successfully processed our requests.\nbike_data &lt;- list()\noffset &lt;- 0\n\nwhile (TRUE) {\n  print(paste(\"Offset:\", offset))\n  result &lt;- tryCatch({\n    get_data(offset)\n  }, error = function(e) {\n    print(paste(\"Error fetching data:\", e$message))\n    return(NULL)\n  })\n  \n  if (is.null(result) || length(result) == 0) {\n    break\n  }\n  \n  bike_data &lt;- c(bike_data, result)\n  offset &lt;- offset + limit\n  \n  # Mindful of rate limits to avoid being blocked by the API\n  Sys.sleep(1)\n}\n\n[1] \"Offset: 0\"\n[1] \"Offset: 1000\"\n[1] \"Offset: 2000\"\n[1] \"Offset: 3000\"\n[1] \"Offset: 4000\"\n[1] \"Offset: 5000\"\n[1] \"Offset: 6000\"\n[1] \"Offset: 7000\"\n[1] \"Offset: 8000\"\n[1] \"Offset: 9000\"\n[1] \"Offset: 10000\"\n[1] \"Offset: 11000\"\n[1] \"Offset: 12000\"\n[1] \"Offset: 13000\"\n[1] \"Offset: 14000\"\n[1] \"Offset: 15000\"\n[1] \"Offset: 16000\"\n[1] \"Offset: 17000\"\n[1] \"Offset: 18000\"\n[1] \"Offset: 19000\"\n[1] \"Offset: 20000\"\n[1] \"Offset: 21000\"\n[1] \"Offset: 22000\"\n[1] \"Offset: 23000\"\n[1] \"Offset: 24000\"\n[1] \"Offset: 25000\"\n[1] \"Offset: 26000\"\n[1] \"Offset: 27000\"\n[1] \"Offset: 28000\"\n[1] \"Offset: 29000\"\n[1] \"Offset: 30000\"\n[1] \"Offset: 31000\"\n[1] \"Offset: 32000\"\n[1] \"Offset: 33000\"\n[1] \"Offset: 34000\"\n[1] \"Offset: 35000\"\n[1] \"Offset: 36000\"\n[1] \"Offset: 37000\"\n[1] \"Offset: 38000\"\n[1] \"Offset: 39000\"\n[1] \"Offset: 40000\"\n[1] \"Offset: 41000\"\n[1] \"Offset: 42000\"\n[1] \"Offset: 43000\"\n[1] \"Offset: 44000\"\n[1] \"Offset: 45000\"\n[1] \"Offset: 46000\"\n[1] \"Offset: 47000\"\n\nraw_bike_data &lt;- bike_data |&gt; \n  map_dfr(as_tibble)\n\n\nSanity Checks - Raw Data\nHurray! Let’s do some sanity checks to assess the data quality.\n\nskim(raw_bike_data)\n\n\nData summary\n\n\nName\nraw_bike_data\n\n\nNumber of rows\n46200\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndate_time\n0\n1\n23\n23\n0\n46200\n0\n\n\ncount\n0\n1\n1\n3\n0\n463\n0\n\n\nmacarthur_ave_display_cyclist_in\n0\n1\n1\n3\n0\n407\n0\n\n\nmacarthur_ave_display_cyclist_out\n0\n1\n1\n3\n0\n319\n0\n\n\n\n\n\nGreat! It’s promising to see 46200 unique datetime values. We’ll need to do some data cleaning and adjust count, macarthur_ave_display_cyclist_in, macarthur_ave_display_cyclist_out to be numeric values. Nevertheless, we can see that the min and max values are presumably between 0 and at most 999. The observed range seems plausible, and the absence of missing values is reassuring.\nThe great thing about the above data retrieval code is that hopefully it remains reproducible once ACT Government releases more recent data! This could be converted into an R package (a later project)."
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#data-cleaning",
    "href": "blog/cycling_in_canberra/index.html#data-cleaning",
    "title": "Cycling in Canberra 🚴",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nLet’s make those minor variable changes. We can now manipulate the data by converting the datetime column to a proper date-time format and ensure numeric columns are actually numeric types.\n\nclean_bike_data &lt;- raw_bike_data |&gt; \n  mutate(\n    date_time = date_time |&gt; \n      str_sub(1, 13) |&gt; \n      as_datetime(format = \"%Y-%m-%dT%H\"),\n    across(\n      !date_time,\n      as.integer\n    )\n  ) |&gt; \n  # Avoid confusion with the count function\n  rename(cyclist_count = count)\nclean_bike_data\n\n# A tibble: 46,200 × 4\n   date_time           cyclist_count macarthur_ave_display_cyclist_in\n   &lt;dttm&gt;                      &lt;int&gt;                            &lt;int&gt;\n 1 2017-11-22 00:00:00            13                                3\n 2 2017-11-22 01:00:00             2                                2\n 3 2017-11-22 02:00:00             0                                0\n 4 2017-11-22 03:00:00             0                                0\n 5 2017-11-22 04:00:00             1                                1\n 6 2017-11-22 05:00:00             9                                5\n 7 2017-11-22 06:00:00            28                               25\n 8 2017-11-22 07:00:00           157                              149\n 9 2017-11-22 08:00:00           358                              315\n10 2017-11-22 09:00:00           144                              131\n# ℹ 46,190 more rows\n# ℹ 1 more variable: macarthur_ave_display_cyclist_out &lt;int&gt;\n\n\n\nSanity Checks - Clean Data\nNow, that the data is cleaned we can do some more sanity checks. The average number of cyclists passing through the Macarthur Avenue crossing is 43.5533766. However, there is quite a right-skewed distribution (mostly low counts by the hour). Interestingly, there’s a slightly more cyclists heading into the city than out of the city. Our dataset ranges between 2017-11-22 and 2017-11-22.\n\nskim(clean_bike_data)\n\n\nData summary\n\n\nName\nclean_bike_data\n\n\nNumber of rows\n46200\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncyclist_count\n0\n1\n43.55\n61.97\n0\n3\n22\n57\n549\n▇▁▁▁▁\n\n\nmacarthur_ave_display_cyclist_in\n0\n1\n22.36\n44.18\n0\n1\n8\n24\n471\n▇▁▁▁▁\n\n\nmacarthur_ave_display_cyclist_out\n0\n1\n21.19\n36.24\n0\n1\n9\n25\n368\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_time\n0\n1\n2017-11-22\n2023-02-28 23:00:00\n2020-07-11 11:30:00\n46200"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#initial-visualisation-explorations",
    "href": "blog/cycling_in_canberra/index.html#initial-visualisation-explorations",
    "title": "Cycling in Canberra 🚴",
    "section": "Initial visualisation explorations",
    "text": "Initial visualisation explorations\nLet’s visualise the hourly cyclist counts over time to identify any discernible trends.\n\nclean_bike_data |&gt; \n  ggplot(aes(\n    x = date_time,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWow, what an ugly plot! It seems like the hourly data has some significant fluctuations (likely nighttime lows). Let’s create a histogram to illustrate the right-skewed distribution of the cyclist_count variable and then aggregate to daily data and plot again.\n\nclean_bike_data |&gt; \n  ggplot(aes(\n    x = cyclist_count\n  )) +\n  geom_histogram(\n    bins = 60\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot below shows the daily cyclist count. Again it’s still quite confusing with a wide range of values.\n\ndaily_cyclist_count &lt;- clean_bike_data |&gt; \n  mutate(\n    date = as_date(date_time)\n  ) |&gt; \n  group_by(date) |&gt; \n  summarise(\n    across(\n      where(is.numeric),\n      sum\n    ),\n    .groups = \"drop\"\n  ) |&gt; \n  ggplot(aes(\n    x = date,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  labs(\n    title = \"Cyclist Count by Date\",\n    x = \"Date\",\n    y = \"Cyclist Count\",\n  ) +\n  theme_minimal()\ndaily_cyclist_count"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#canberrans-cycle-more-during-the-week",
    "href": "blog/cycling_in_canberra/index.html#canberrans-cycle-more-during-the-week",
    "title": "Cycling in Canberra 🚴",
    "section": "Canberrans cycle more during the week",
    "text": "Canberrans cycle more during the week\nLet’s take the same plot and add some colour indicating the day of the week. Now this paints a better picture, we’ve discovered that Canberrans are cycling less over the weekend. This also brings out the seasonality in the dataset (perhaps Summer and Spring). But we also can see the impacts of the COVID-19 lockdowns and potentially the aftermath of working from home policies.\n\ndaily_cyclist_count_coloured &lt;- daily_cyclist_count +\n  aes(colour = wday(date, label = TRUE)) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Day of Week\") +\n  theme(legend.position = \"bottom\")\ndaily_cyclist_count_coloured"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#morning-and-afternoon-commutes",
    "href": "blog/cycling_in_canberra/index.html#morning-and-afternoon-commutes",
    "title": "Cycling in Canberra 🚴",
    "section": "Morning and Afternoon Commutes",
    "text": "Morning and Afternoon Commutes\nWe’ve focused on the days of the week, let’s now visualise the time of day and also facet by day of the week.\n\ncyclist_count_by_hour &lt;- clean_bike_data |&gt; \n  mutate(\n    hour = hour(date_time)\n  ) |&gt; \n  group_by(hour) |&gt; \n  summarise(\n    across(\n      where(is.numeric),\n      mean\n    ),\n    .groups = \"drop\"\n  ) |&gt; \n  ggplot(aes(\n    x = hour,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Average Cyclist Count by Hour\",\n    x = \"Hour of the Day (24 hour time)\",\n    y = \"Cyclist Count\",\n  ) +\n  theme_minimal()\ncyclist_count_by_hour\n\n\n\n\n\n\n\n\nMorning and afternoon work commutes! It also seems there’s an increase around 3pm, maybe the finishing of school!\nLet’s facet by day of the week to compare the hours by their respective days.\n\nclean_bike_data_hourly_by_day_of_the_week &lt;- clean_bike_data |&gt;\n  mutate(\n    hour = hour(date_time),\n    day_of_week = date_time |&gt; \n      date() |&gt; \n      wday(week_start = 1, label = TRUE)\n  ) |&gt;\n  group_by(hour, day_of_week) |&gt;\n  summarise(\n    across(\n      where(is.numeric),\n      mean\n    ),\n    .groups = \"drop\"\n  )\n\ncyclist_count_by_hour_with_facet &lt;- clean_bike_data_hourly_by_day_of_the_week |&gt; \n  ggplot(aes(\n    x = hour,\n    y = cyclist_count\n  )) +\n  geom_point() +\n  geom_line() +\n  labs(\n    title = \"Average Cyclist Count by Hour, Faceted by Day of Week\",\n    x = \"Hour of the Day (24 hour time)\",\n    y = \"Cyclist Count\"\n  ) +\n  theme_minimal() +\n  facet_wrap(~day_of_week)\n\ncyclist_count_by_hour_with_facet\n\n\n\n\n\n\n\n\nVery interesting, as expected from the previous plots, weekends have less cyclists. We can see the morning work commutes during the week; however, there seems to be less commuters on Mondays and Fridays."
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#next-steps",
    "href": "blog/cycling_in_canberra/index.html#next-steps",
    "title": "Cycling in Canberra 🚴",
    "section": "Next Steps",
    "text": "Next Steps\nMoving forward, it would be interesting to expand the analysis and incorporate weather information and maybe major events like Spilt Milk or Groovin. I also want to look into the effect of Public Holidays and perhaps see if there’s any late night cyclists coming home from Thursday, Friday, Saturday nights out from Civic.\nAnother prospective idea would be looking into air quality data.\nCould even look into doing some time series forecasting with the tidymodels or fable R packages.\nHappy Cycling! 🚴"
  },
  {
    "objectID": "blog/cycling_in_canberra/index.html#resources-used",
    "href": "blog/cycling_in_canberra/index.html#resources-used",
    "title": "Cycling in Canberra 🚴",
    "section": "Resources used:",
    "text": "Resources used:\n\nACT Bike Barometer - MacArthur Avenue\nGreat video by Melissa Van Bussel (ggnot2) explaining how APIs work in R\nRaelene analysed the same dataset (2018 - 2020)\nHelped me figure out what offset and limit parameters do"
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html",
    "href": "blog/bikepacking_attack_of_the_buns/index.html",
    "title": "Bikepacking - Attack of the Buns",
    "section": "",
    "text": "Attack of the Buns…. Or perhaps this blog post should be called Attack of the Braids, because we began the adventure from Braidwood, instead of the usual start in Bungendore. This blog post is all about my recent bikepacking trip in early 2025. We started in Bungendore travelling through Morton National Park past Nowra and Kangaroo Valley finishing in Moss Vale."
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#day-0-car-logistics",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#day-0-car-logistics",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Day 0: Car Logistics",
    "text": "Day 0: Car Logistics\nThe trip “began” after work on a Wednesday afternoon. The plan was for Robin and I to drive separately from Canberra to Moss Vale. We would then “scissors, paper, rock” to determine whose car would be left in Moss Vale, then grab Subway and back home. With our bikes and gear now all packed in the car, it was time for an early nights sleep.\nThe plan from here would be to drive to Braidwood, then cycle across 4 days back to the car in Moss Vale and then collect the other car in Braidwood and head back to Canberra. The alternatives would have been getting someone to pick us up from Moss Vale or catching the train back to Canberra. Having overestimated our bikepacking speed in the past, the convenience of having our own transportation post-bike leg was very worth the Wednesday evening effort."
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#day-1-braidwood-to-piercys-clearing",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#day-1-braidwood-to-piercys-clearing",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Day 1: Braidwood to Piercys Clearing",
    "text": "Day 1: Braidwood to Piercys Clearing\n\nThe Bike Pump…\nToday was the first day where (almost) everything went to plan - unfortunately a rariety in my bikepacking career so far.\nAwake and ready to start the day, we would make our way straight to Braidwood Bakery1. The bikes and bags are now fully setup and we’ve taken our first of many 10 second self-timer photos. Robin wants to quickly pump up his tires before we hit the gravel. To our surprise, I realise that I had forgotten to bring the valve adaptor for our pump2.\n1 Braidwood Bakery is such a local classic for any Canberrans heading to the coast for the weekend.2 Most modern bikes use Presta valves, whereas cars use Schraeder valves, unfortunately, my handpump only had the Schraeder valve. A rookie mistake!\n\n\n\n\n\n\n\n\nPlaying bike tetris\n\n\n\n\n\n\n\nBraidwood Bakery\n\n\n\n\n\n\n\n\n\nCompulsory start photo\n\n\n\n\n\nIn a worried manner, we push our bikes through town asking shopkeepers if they sell any pumps:\n\nThe local outdoor shop … negative\nThe petrol station … negative\nThe local IGA … negative\nThankfully, Braidwood Mitre 10 came in clutch with a full-sized bike pump\n\nWe strap it onto my front handle bars with some spare straps I brought (a reminder to always bring some spare straps!).\n\n\n\n\n\n\n\n\n\nBike pump acquired\n\n\n\n\n\n\n\nThank you, Braidwood Mitre 10\n\n\n\n\n\n\n\nPumped up, let’s go, off to Nerriga Pub!\nAnd off we go having already problem solved, we jump onto Little River Road leading onto Charleys Forest Road off to Nerriga. The gravel was smooth and slightly undulating. Complimented by a cool misty morning, we were flying past grazing cattle, calm creek crossings, and a peaceful snake enjoying the warm road!\n\n\n\nPOV: Robin taking a photo of the snake\n\n\nWith the sun starting to break through the clouds, arriving at Nerriga was a lifesaver from the incoming onslaught of the heat. Excited to down a celebratory pint of Carlton Draught and a pub feed, we were greeted by two confused looking locals joking they would have preferred taking their motorbikes instead of bicycles. I followed with a slight nod and a chuckle, as I gazed over to see another bikepacker enjoying the shade. Not able to wait any longer, I rush inside to see a bustling pub full of elderly people enjoying their meals. I order the pub burger and my pint and head outside curious to hear where the other bikepacker had come from.\nLachie had the full bike setup, a bunch of bike bags, mountain bike cleets, thick tires, and even some tri bars. I knew I was just about to meet an expert in the field. Offering a seat to his table, we had lunch with him finding out he had just come from Gerringong via the same route we had planned - only to realise his final destination was not Braidwood, but in fact Tasmania…. After exchanging stories and advice, Robin and I left for our afternoon session.\n\n\n\n\n\n\n\n\n\nHighly recommend the pub burger\n\n\n\n\n\n\n\nInside Nerriga Hotel\n\n\n\n\n\n\n\n\n\nReady for the afternoon cycle\n\n\n\n\n\n\n\nEndrick River swim\nBack on gravel, we came across Endrick River, a lovely swimming spot by a small shady forde. Perfect timing, plenty of daytime to spare, so a quick dip was so refreshing! Drying off, we’re back on the bikes cycling through Morton National Park overlooking some impressive cliff faces following the Red Ground Track3. After a small mishaps in navigation4, we arrived at Piercy’s Clearing about 100m from a smaller estuary of Endrick River. Another quick rinse off followed by some Backcountry freeze-dried Satay Chicken and some much needed stretching, we were ready for sleep.\n3 Beware this Track is super rocky, like big chunky sharp rocks. I’m surprised we didn’t get a flat!4 Note to self: Always double-check the map, especially after a 35°C day of cycling 🥵\n\n\n\n\n\n\n\n\nEndrick River Swim\n\n\n\n\n\n\n\nPiercy’s Clearing\n\n\n\n\n\n\n\n\n\nCan never have enough ‘bike leaning against stuff’ photos\n\n\n\n\n\n\n\nCliffs in the distance\n\n\n\n\n\n\n\nSome night time reflection\nLaying on my mat resting my head against my rolled up sleeping bag. The current sleeping arrangements are a cosy 1-person tent with a luxuriously large Sea to Summit sleeping mat. Planning on using my sleeping bag liner and keeping the bag packed up given the a warm-ish night. I reflect through photos and videos of the day feeling my joints relax and recover after a warm days ride.\nListening to the rustling shoulders against the mat searching for the optimal position, I finally get comfortable and wrap myself in the liner. Immediately, I feel the merino radiating keeping me warm and snug and slowly close my eyes to the whispers of crickets and creaking nocturnal animals in the distance. Hopefully I still have all my food in the morning!\n\n\n\nGood night 😴"
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#day-2-piercys-clearing-to-nowra",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#day-2-piercys-clearing-to-nowra",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Day 2: Piercy’s Clearing to Nowra",
    "text": "Day 2: Piercy’s Clearing to Nowra\nBreakfast is always a struggle! Consistent food intake is so so important when it comes to endurance events. So when I know there’s a long, sunny day ahead, I’ll attempt to eat as much as I can for breakfast and essentially every other meal of the day.\n\n\n\n\n\n\nToday’s planned menu:\n\n\n\n\nBreakfast (at Piercy’s Clearing): Porridge, nutella, salami, hazelnut coffee\nLunch (at Yalwal): Backcountry Moroccan Lamb\nDinner (in Nowra): Pizza, Bundaberg Guava\nDessert (at the motel): Maxibon, chocie milk\nSnacks (constantly): Peanut M&Ms, salted nuts, soya Crisps, Gu gels5, powerade\n\n\n\n5 Big fan of the flavourless Nude “flavour”. Strawberry and Banana & Birthday Cake are pretty good too. I prefer thicker gels, so Gu works well for me.\nRocky flats & sandy downhills\nToday’s first few kilometres are met with large rocky sections making a very slow start. I’m surprised we didn’t get any flats as it was very technical riding on our thin gravel tires. We finally made it out of that “hell-ish”6 rocky section to arrive on top of the clouds overlooking the valley we’d just been sleeping in. It’s an awesome feeling looking out into the distance on the cliff tops, feeling a slight breeze on your face surrounded by eucalypts watching the fire trail dance along the ridgeline. This was one of the highlights.\n6 We met another bikepacker racing in the Sydney to Summit race, who greeted us with “welcome to hell”, as he was flowing down the mountain. I think he was in a lot of pain, the kind of pain that is felt once you’re truly past type 2 fun. I think I found his Strava after the trip; he made it to Kosciuszko.\n\n\n\n\n\n\n\n\nI’ve really enjoyed having Cucumbers on long rides, it’s like solid water that doesn’t melt or get overly warm\n\n\n\n\n\n\n\nThe rocks from hell\n\n\n\n\n\n\n\n\n\nMorton National Park - The Vines\n\n\n\n\n\n\n\nHow cool is epicormic growth - resprouting eucalypts after bushfire damage\n\n\n\n\n\nBack into action, we get back onto tarmac for a sweet 75km/h+ speedy descent down Braidwood Road7 shortly after jumping back onto the gravel following Yarramunmun Firetrail. Now we had switched from rocky technicals to the sandy downhills, from one technical to the next. Whilst the downhill was appreciated, the heat reflecting off the white-beige sand radiated for what felt for a few monotonous hours.\n7 Going fast is just such a thrill, but wow it’s also just so so dangerous.8 I can’t believe there are some people that opted to do Attack of the Buns the opposite direction and would have to climb up that section, absolutely bonkers.Thankfully, after an incredibly steep descent8, we finally made it into Yalwal, a camping green space next to one of several dams around the Shoalhaven region. Seeking out shelter from the blistering sun, we spent over 2 hours eating, playing cards, attempting to rehydrate and recover in preparation for a big climb out of the basin. After a backcountry, way too many M&Ms, several chugs of my warm Powerade, we were as ready as could be for the climb.\n\n\n\n\n\n\n\n\n\nMorton National Park - The Vines\n\n\n\n\n\n\n\nMoo!\n\n\n\n\n\n\n\n\n\nMorton National Park - The Vines\n\n\n\n\n\n\n\nThe Fun Scale - “this is fun, that was fun, that sucked”\nPowering up a mountain is a mental game, a tug of war between pushing through the lactic acid buildup or stopping and catching a break in the shade. The latter is promising, but the ego says otherwise. At some point, the body gets used to the pain, sometimes even reaching the nirvana-state of type 2 fun - miserable in the moment, but awesome in retrospect. Thoughts slowly get filled with echoes of doubt, but also reminders of triumph and motivation9. The grit comes out and you power through sometimes even zoning out or perhaps it’s actually zoning in; into a state of mind where the only objective is “push” and hope you don’t end up in type 3 fun10. Enough talk about the fun scale, back to the day.\n9 There’s definitely a motivation to push through the pain to ensure you maximise your Strava statistics. Anything for the forward-looking expectation of the kudos-induced dopamine hit! ⚡10 Type 1 fun is fun in the moment and in retrospect. Type 2 fun is not fun in the moment, but fun in retrospect (this is probably the most prominent feeling for long-distance runners). Finally, type 3 fun is not fun in the moment and also not fun in retrospect - “it just sucks”.Making it up to the top, I see a sun-protected area, whip out my luxury-item - foldable cushioned mats - sit back and enjoy the last drops of my lukewarm powerade. I’m running low on snacks, I’m excited for Nowra. Robin and I enjoy some more shade listening to our nostalgic 2010s (arguably emo/edm phase) music, hop on the saddle heading east to Nowra.\n\n\n\n\n\n\n\n\n\nMade it to the top\n\n\n\n\n\n\n\nProfusely sweating\n\n\n\n\n\n\n\nHow good is Chocie Milk ❤️\nGetting back into a busy town, especially over the summer weekend, was a little bit of a shock. So much stopping and starting. We get into Nowra CBD and I propose to Robbie, that I quickly run into Coles to grab a 2L Oak chocie milk… Best decision. I had about a whole liter within a few minutes.\nGetting to the motel made me really appreciate all the common household services, like a cold shower, a microwave, a bed, an aircon (omg the aircon was immediately turned on upon entering!), a chair and table, fresh drinkable cold water11. What luxuries.\n11 As someone who’s only recently gotten into camping, this must be a common experience to those more experienced, but alas a nice personal appreciation.We finished off the day with some local woodfired pizzas and restocking on snacks at Coles. Returning to the aircon-chilled room and covering up under blankets was a lovely change (can’t believe I appreciated this so much given it was only one night of camping eeeep).\n\n\n\n\n\n\n\n\n\nChocie Milk!\n\n\n\n\n\n\n\nDinner"
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#day-3-nowra-to-griffins-farm-campground",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#day-3-nowra-to-griffins-farm-campground",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Day 3: Nowra to Griffin’s Farm Campground",
    "text": "Day 3: Nowra to Griffin’s Farm Campground\nDay 3 was another hot day. Climbing out of Nowra, we made our way to Kangaroo Valley via Mount Scanzi. This time with newly stocked snacks and a good nights sleep. The morning ride only took a few hours, so we ended up taking a lengthy break in Kangaroo Valley. First, an iced coffee, then a beer, then another beer + a mushroom burger at the local pub.\nA great contrast to the previous two days’ rides. Dermot and Lyla ended up driving and meeting us for lunch and an afternoon swim under the Kangaroo Valley bridge. A shady and refreshing place to cool off from the steamy sun.\n\n\n\n\n\n\n\n\n\nYou would not believe your eyes. If ten million fireflies. Lit up the world as I fell asleep\n\n\n\n\n\n\n\nUnfortunately, the Mars bar fell out on one of the descents\n\n\n\n\n\n\n\n\n\nUnfortunately, I lost the Mars bar on one of the descents 😔\n\n\n\n\n\n\n\nAt the top of Mount Scanzi\n\n\n\n\n\n\n\n\n\nJust enough water to call it a bath\n\n\n\n\n\nThe afternoon was another short ride to Griffin’s Farm Campground, a lovely open clearing next to a flowing creek in the bottom of the valley. A refreshing evening swim cooldown, another Backcountry meal, and finish off the day with some 500! Thankfully, Robin tossed his headtorch into the distance distracting all the bugs, which created some interesting night photography."
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#day-4-griffins-farm-campground-to-moss-vale-to-home",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#day-4-griffins-farm-campground-to-moss-vale-to-home",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Day 4: Griffin’s Farm Campground to Moss Vale to Home",
    "text": "Day 4: Griffin’s Farm Campground to Moss Vale to Home\nThe last day was another shorter ride. A lot of uphill, around 550m consistent climbing, so a lengthy breakfast was necessary. Good thing Lyla brought these extreme electrolyte powder, which we calculated to be about 3 powerade bottles worth. Whilst the morning mist meant we finally had a day without direct sunlight, but this brought with it its own set of challenges… humidity.\nWe bid our farewells to the (car)packers and began the climb. We cycled past a runner, who apparently started from the top of the hill, this would have been around 1200m of vert over 22km - how wild!. After pressing on and passing Gales Flat Campground, we finally made it to Meryla Pass (the top of the mountain).\nA few more country roads and farm properties, we made it back to Moss Vale. Packed up the bikes in the car and grabbed some more food. An iced chocolate, bacon & egg roll, and chips!\n\n\n\n\n\n\n\n\n\nWe always start the day with some hot hazelnut powder coffee\n\n\n\n\n\n\n\nFinisher’s Reward\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparently this is a really beautiful view\n\n\n\n\n\n\n\nForests and more forests\n\n\n\n\n\n\n\n\n\nGriffins Farm Campground\n\n\n\n\n\n\n\nFinish line!\n\n\n\n\n\nOverall, the trip was very enjoyable, plenty of laughs, a touch too much heat, but great company and views. Looking forward to the next bikepacking adventure!"
  },
  {
    "objectID": "blog/bikepacking_attack_of_the_buns/index.html#appendix-photos",
    "href": "blog/bikepacking_attack_of_the_buns/index.html#appendix-photos",
    "title": "Bikepacking - Attack of the Buns",
    "section": "Appendix Photos",
    "text": "Appendix Photos\nThese are the photos that didn’t make it…\n\n\n\nMailboxes between Braidwood and Nerriga\n\n\n\n\n\nCow\n\n\n\n\n\nBurnt Banksia\n\n\n\n\n\nToilet"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "I write about a bunch of different things…\nI hope you find something useful here! 😊\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nReflections on my Jawun Secondment\n\n\n\n\n\n\nWork\n\n\nReflections\n\n\n\nIt’s not basic, it’s foundational…\n\n\n\n\n\nJun 19, 2025\n\n\nJohann Wagner\n\n\n\n\n\n\n\n\n\n\n\n\nSSA Mentoring with Emi (April 2025)\n\n\n\n\n\n\nMentoring\n\n\n\nSetting up the goal posts, \n\n\n\n\n\nApr 2, 2025\n\n\nJohann Wagner\n\n\n\n\n\n\n\n\n\n\n\n\nCycling in Canberra 🚴\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nCycling\n\n\nCanberra\n\n\n\nInitial exploration into Canberra’s cycling habits\n\n\n\n\n\nMar 18, 2025\n\n\nJohann Wagner\n\n\n\n\n\n\n\n\n\n\n\n\nBikepacking - Attack of the Buns\n\n\n\n\n\n\nBikepacking\n\n\nAdventure\n\n\n\nA hot 4-day bikepacking adventure exploring Australia’s Southern Tablelands\n\n\n\n\n\nMar 15, 2025\n\n\nJohann Wagner\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify Exploratory Data Analysis - Streaming History Data\n\n\n\n\n\n\nData Projects\n\n\nExploratory Data Analysis\n\n\nSpotify\n\n\n\nMy Spotify streaming history throughout July 2022/2023\n\n\n\n\n\nDec 29, 2023\n\n\nJohann Wagner\n\n\n\n\n\n\n\n\n\n\n\n\nInvasive Animal Species Dashboard\n\n\n\n\n\n\nData Projects\n\n\nExploratory Data Analysis\n\n\nUniversity Assignment\n\n\n\nUniversity Assignment: Developing an RShiny dashboard to visualise invasive animal species in Australia\n\n\n\n\n\nOct 20, 2023\n\n\nJohann Wagner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/jawun_secondment/index.html",
    "href": "blog/jawun_secondment/index.html",
    "title": "Reflections on my Jawun Secondment",
    "section": "",
    "text": "“It’s not basic, it’s foundational.” This statement truly summarises my time at the South West Aboriginal Medical Service (SWAMS). My transformative six-week Jawun Secondment was an experience that has profoundly solidified my belief that relationships, trust, and human skills are absolutely crucial for data professionals."
  },
  {
    "objectID": "blog/jawun_secondment/index.html#what-is-the-jawun-secondment-program",
    "href": "blog/jawun_secondment/index.html#what-is-the-jawun-secondment-program",
    "title": "Reflections on my Jawun Secondment",
    "section": "What is the Jawun Secondment Program?",
    "text": "What is the Jawun Secondment Program?\nJawun is a unique program that partners corporate, government, and not-for-profit organisations with Indigenous communities and organisations across Australia. It’s all about bringing specific skills and expertise to where they’re most needed, to support Indigenous-led initiatives and goals. I had the incredible opportunity to step away from my usual data analysis/data science role and immerse myself in one of these partnerships."
  },
  {
    "objectID": "blog/jawun_secondment/index.html#cultural-immersion-listening-and-learning-on-country",
    "href": "blog/jawun_secondment/index.html#cultural-immersion-listening-and-learning-on-country",
    "title": "Reflections on my Jawun Secondment",
    "section": "Cultural Immersion: Listening and Learning on Country",
    "text": "Cultural Immersion: Listening and Learning on Country\nThe first week of any Jawun secondment is dedicated to cultural immersion, and honestly, it’s designed to slow things down, be a sponge and soak up all the deep listening. It was a whirlwind of learning, listening, and experiencing - connecting with Country. We began week one at South West Aboriginal Land and Sea Council doing some general Jawun induction and learning more about Noongar Boodja Country. More learning continued at the incredible WA Museum Boola Bardip, where the exhibits powerfully articulate the rich history, resilience, and ongoing connection to Country of Noongar people.\n\n\n\nMap of Noongar Boodja\n\n\nWe then visited several local Indigenous organisations1. Seeing firsthand the incredible work being done on the ground – the holistic approaches to delivering culturally appropriate healthcare and support – was truly inspiring.\n1 Derbarl Yerrigan Health Service, Moorditj Koort, and Boola Bidi (Harvey Aboriginal Corporation)\n\n\nDerbarl Yerrigan means Swan River in Noongar\n\n\nPerhaps one of the most profound parts of the immersion was spending two nights camping in swags at Roelands Village, a former Aboriginal mission for Stolen Generation children2. Being on that land, hearing the stories shared by Syd Jackson and community members, was very special. It was a stark reminder of resilience, healing, and the ongoing impact of historical, colonial policies3.\n2 The Stolen Generations refers to the Aboriginal and Torres Strait Islander children who were forcibly removed from their families by Australian government agencies and church missions between the 1910s and 1970s.3 Syd’s story is truly incredible, I highly suggest reading his story\n\n\nCamping at Roelands Village\n\n\nWe also explored the artistic and spiritual dimensions of the region, visiting the breathtaking Wellington Dam mural. Week one wasn’t just about absorbing information; it was about truly connecting and building deep respect for the communities we’d be working with.\n\n\n\nWellington Dam: the biggest dam mural in the world\n\n\nThis deep dive into culture and community was reinforced in our third week, where we spent the day experiencing the Ngligi Caves with Koomal Dreaming. We learnt about Aboriginal bushcraft and about the Noongar seasonal calendar focusing on six Noongar seasons4, instead of the western/european four seasons! These experiences deepened my understanding of traditional bushcraft and respect for Country.\n4 The six Noongar seasons are Birak, Bunuru, Djeran, Makuru, Djilba, and Kambarang. Each corresponds to specific environmental and weather patterns, influencing traditional food gathering and land management.\n\n\nThis is Josh, owner of Koomal Dreaming. Fire has an incredible cultural importance for Noongar people."
  },
  {
    "objectID": "blog/jawun_secondment/index.html#the-pivot-shifting-from-a-technical-plan-to-a-human-centred-approach",
    "href": "blog/jawun_secondment/index.html#the-pivot-shifting-from-a-technical-plan-to-a-human-centred-approach",
    "title": "Reflections on my Jawun Secondment",
    "section": "The Pivot: Shifting from a Technical Plan to a Human-Centred Approach",
    "text": "The Pivot: Shifting from a Technical Plan to a Human-Centred Approach\nAfter that intense and invaluable first week, it was time to put our skills to work. I was incredibly fortunate to be placed with SWAMS in Bunbury. SWAMS is a vital organisation providing holistic health services to Aboriginal people across Noongar Boodja. My initial project brief was to help them establish a unified organisational data model5.\n5 I had never heard this term before. It’s basically a single source of truth for an organisation’s data.My approach was to start having conversations with everyone in the team. Learn their pain points, listen to their stories. I found this created a good foundational relationship and trust. However, the more and more I learned about the team, the more I realised that a unified organisational data model was probably inappropriate for their current data maturity. This lead me to focus on establishing a data management and data governance policies.\nA lot of this adjustment in approach was guided through conversations with previous work colleagues from ACT Health and current work colleagues back at Jobs and Skills Australia. This was about more than just networking; it was about empowering the SWAMS team by showing them a broader community willing to share knowledge, validating their challenges, and providing insights into how other organisations tackle similar data problems. I’m super grateful for this human-approach to problem-solving6, as this lead me to adjust my project brief.\n6 Or more accurately, problem-identifying.\n\n\nSunset over the Indian Ocean at Bunbury"
  },
  {
    "objectID": "blog/jawun_secondment/index.html#co-designing-the-foundation",
    "href": "blog/jawun_secondment/index.html#co-designing-the-foundation",
    "title": "Reflections on my Jawun Secondment",
    "section": "Co-Designing the Foundation",
    "text": "Co-Designing the Foundation\nMy five weeks with SWAMS were focused on helping them build stronger foundations in their data capabilities. It wasn’t about quick fixes; it was about co-designing sustainable solutions that would empower them long after I left. Here’s a rundown of what we managed to achieve together.\n\nEstablishing a Data Analytics Service\nThe cornerstone of my secondment was co-designing the Data Analytics Service Framework. This wasn’t just me ‘building’ a document; it meant plenty of yarning7, deep listening, and iterating with the SWAMS team to ensure it genuinely met their unique needs. The framework defined:\n7 In an Aboriginal context, ‘yarning’ is more than just a chat. It’s a culturally prescribed, collaborative form of dialogue used for storytelling, learning, and building relationships.\nScope (Why?): What data analytics could and should be done, and crucially, why it mattered for their mission.\nWork Plan (What?): A clear roadmap of specific data analytics activities and projects they could undertake.\nService Charter (How?): Guidelines for how the data analytics service would operate, ensuring clarity and consistency.\n\nThis framework is a powerful strategic tool. It’s helped SWAMS gain immense strategic clarity by defining responsibilities, enabling a shared understanding of data across the organisation, and supporting more sustainable resource allocation. Now, everyone has a clearer picture of who does what, making data work more efficient and impactful.\n\n\nData Roadmap\nThe roadmap we developed isn’t a rigid plan, but more like a compass – a guide for SWAMS to navigate their journey towards better data practices and maturity over time8. It helps them prioritise where to focus their efforts. The roadmap follows the Data Management Body of Knowledge (DAMA-DMBOK) structure, which helped categorise my recommendations9.\n8 Candidly, the roadmap became a document containing everything I wanted to do but didn’t have time for. It was intentionally left rough to allow for flexibility.9 The three essential pillars I focused on were People (Data Culture), Processes (Data Management), and Technology (Tools).\n\n\nThe analogy: We’ve shifted from being lost in fog, to still being lost, but at least we have a compass, some guidance through the mist!\n\n\n\n\nEnhancing Data Capability\nKnowing that building internal capability was key, I led a series of capability workshops on Reproducible Data Workflows in R. This wasn’t just about teaching code; it was about empowering the team to conduct their analysis in a way that is transparent, verifiable, and easily repeatable. We talked about everything from basic R functions to the importance of reproducibility. This session really started from the groundup and was aimed as a spark of inspiration for the team to improve their processes.\n\n\n\nOne of my handouts for the workshop"
  },
  {
    "objectID": "blog/jawun_secondment/index.html#next-steps-sustaining-the-momentum",
    "href": "blog/jawun_secondment/index.html#next-steps-sustaining-the-momentum",
    "title": "Reflections on my Jawun Secondment",
    "section": "Next Steps: Sustaining the Momentum",
    "text": "Next Steps: Sustaining the Momentum\nThe secondment might be over, but the work of building strong data foundations at SWAMS continues. We laid the groundwork, and now it’s about nurturing that growth:\n\nEmbed & Socialise the Service Framework: This framework needs to become part of SWAMS’ daily operations.\nUse the Data Roadmap as a compass: Guiding them towards continuous ‘better practice.’\nContinue learning to implement reproducible workflows: Ensuring ongoing data quality and efficiency.\nFormalise Data Management & Governance: Strengthening integrity, security, and compliance.\nChampion Data Culture and Data Literacy: Creating an environment where everyone understands and appreciates the potential value of data."
  },
  {
    "objectID": "blog/jawun_secondment/index.html#what-the-secondment-meant-to-me-personal-reflections",
    "href": "blog/jawun_secondment/index.html#what-the-secondment-meant-to-me-personal-reflections",
    "title": "Reflections on my Jawun Secondment",
    "section": "What the Secondment Meant to Me: Personal Reflections",
    "text": "What the Secondment Meant to Me: Personal Reflections\nThis secondment wasn’t just about delivering projects; it was a profound experience of personal and professional growth. It’s tricky to quantify the full impact of these six weeks. So much of the value lies not in a final report, but in the ripple effect of new ideas, the spark of empowerment in my colleagues, and the slow, steady work of building trust. The real impact will reveal itself over time, but the impact on me is already crystal clear.\n\n\n\nYarning by the campfire at Roelands Village\n\n\n\nThe Unshakeable Power of Relationships & Trust\nThe co-design process truly highlighted the power of collaboration. My approach was to sit and listen, to understand their challenges from their perspective, and to build genuine relationships. This had major dividends in the later half of my secondment, in turn leading to easy buy-in for the frameworks and workshops I was introducing. I know I built effective relationships with leaders like Aneliese and Devin, who proactively checked in with me, shared stories about work and outside of work. It wasn’t just a consultation; it was a true partnership.\nThis trust created moments of true skills transfer. When my colleague Keely came to me with an Excel question, I helped her debug and find a solution. But more importantly, I was able to show her how I troubleshoot and my methods for problem-solving. That moment of shared learning was only possible because a foundation of rapport and trust had already been built.\n\n\nCombatting My Imposter Syndrome & Building Self-Confidence\nThis secondment was a huge personal victory against my own imposter syndrome. In my regular role, surrounded by equally skilled data professionals, it’s easy to feel like you’re just one cog in a very smart machine. But here, in a different context, I realised: “Yes, I do have relevant skills, and I can add significant value. People genuinely value my contributions and my way of thinking.” That was a powerful ‘aha!’ moment for me.\n\n\nShifting Mindsets: From ‘Best’ to ‘Better’ Practice\nMy usual work environment operates at a relatively high level of data maturity. Being at SWAMS gave me invaluable insight into organisations at earlier stages of their data journey and forced me to re-evaluate what ‘best practice’ truly means. I quickly saw that many organisations, regardless of size or sector, grapple with similar data challenges—a powerful reminder that “you’re not alone” in the data struggle.\nThis led to a key personal learning: my mindset shifted from chasing ‘best practice’ to embracing ‘better practice.’ It’s about continuous improvement and finding what works for that specific organisation in their context, rather than imposing a one-size-fits-all ideal. Sometimes, the most sophisticated solutions aren’t the best. A simple Excel spreadsheet, used effectively, can do wonders and be transformative for an organisation just starting its data maturity journey. This is precisely why ‘It’s not basic, it’s foundational’ fits so well – what might seem simple to me could be a game-changer for others.\n\n\nEarly Career Leadership Development\nMy usual role is heavily technical – deep in the code, building dashboards, and automating reports. But these past six weeks were incredibly strategic and tactical. I wasn’t just executing; I was applying strategic thinking, problem-solving at a higher level, and guiding conversations to shape future directions. This was a fantastic opportunity for early career leadership development, pushing me into a more advisory leadership role.\n\n\nEmpowering Leadership: People Over Tools\nThis secondment reinforced a core belief of mine: while I’m passionate about data tools and technologies, I care more deeply about people. My role wasn’t just to implement software; it was to empower individuals, to build their confidence, and to spark their own curiosity and capability. It’s not about doing the work for them; it’s about igniting that internal ‘flame’ so they can continue to grow and tackle their own data challenges independently.\n\n\nAcknowledging the Foundational Nature of the Work\nI sometimes feel like I haven’t done anything huge, but then I take a step back and reflect. It may seem like it’s been relatively slow, but at least we’re moving in the right direction (at least we have a direction now!). In six weeks, you don’t build a skyscraper. You lay the concrete slab, you dig the footings, you ensure the ground is stable. It might not look ‘big’ on the surface, but it’s absolutely essential for whatever comes next.\n\n\n\n\n\n\nThank You / Yanga\n\n\n\nNo journey like this is taken alone. I want to extend my deepest gratitude to everyone who made this experience so impactful.\n\nTo the entire South West Aboriginal Medical Service (SWAMS) team – thank you for welcoming me, for your trust, and for your incredible dedication to your community. It was an honor to work alongside you.\nTo the Jawun program – thank you for creating these invaluable opportunities for growth, learning, and connection.\nTo my colleagues Jakob, David, Imran, and Matt – thank you for your support, valuable advice, and for indulging my ‘nerd chats’ from afar.\nSpecial thanks to Devin and Aneliese at SWAMS for their leadership, openness, and for trusting me with such important work. Your partnership made all the difference. I look forward to hearing about all the future progress with your team!\nAnd to Aaron, for being an awesome support/sounding board. I look forward to our future phone call chats!\n\nThis secondment has truly been a journey of not just doing, but of being – deeply listening, building trust, and contributing to foundations that will (hopefully) empower for years to come.\n\n\nYanga10.\n10 ‘Yanga’ is the Noongar word for ‘thank you’ or ‘acknowledgement’."
  },
  {
    "objectID": "blog/jawun_secondment/index.html#appendix-photos",
    "href": "blog/jawun_secondment/index.html#appendix-photos",
    "title": "Reflections on my Jawun Secondment",
    "section": "Appendix Photos",
    "text": "Appendix Photos\n\n\n\nFood at Koomal Dreaming - lots of native plants and animals\n\n\n\n\n\nSelf-guided wine tour on our bikes at Margaret River!\n\n\n\n\n\nEven got some great father-son time cycling at Seven Gravel Race"
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "",
    "text": "The ANU CBE Building 26C is one of my more iconic memories of my time at university as a student.1 It’s a great study place if you can find any free/empty tables. This building is also home to a small coffee shopfront in the main foyer with plentiful tables to choose from. This is where Emi and I decided to meet for our 2nd SSA mentoring session.\n1 One of my first tutorials was in this building and I remember frantically trying to arrive sharp and early with a textbook in hand and laptop in the other."
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#our-2nd-ssa-mentoring-session-at-cbe-buidling-26c",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#our-2nd-ssa-mentoring-session-at-cbe-buidling-26c",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "",
    "text": "The ANU CBE Building 26C is one of my more iconic memories of my time at university as a student.1 It’s a great study place if you can find any free/empty tables. This building is also home to a small coffee shopfront in the main foyer with plentiful tables to choose from. This is where Emi and I decided to meet for our 2nd SSA mentoring session.\n1 One of my first tutorials was in this building and I remember frantically trying to arrive sharp and early with a textbook in hand and laptop in the other."
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#purpose-blog-writing-helps-achieve-one-of-my-goals",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#purpose-blog-writing-helps-achieve-one-of-my-goals",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "Purpose: Blog writing helps achieve one of my goals",
    "text": "Purpose: Blog writing helps achieve one of my goals\nI’m using this blog to document my mentoring sessions, primarily to improve my writing. But it’s also about preserving those ‘aha!’ moments, those conversations that are too good to forget, 2. Writing them down is a perfect way to do that, and a great opportunity to get better at using Quarto.\n2 Now that I’m writing this, I think I’m realising I’ve already forgotten much of our conversations (even though it’s only been a bit over a day since meeting). Perhaps I should record the sessions… a podcast 😀"
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#wow-git-can-be-so-confusing-sometimes",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#wow-git-can-be-so-confusing-sometimes",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "Wow, git can be so confusing sometimes",
    "text": "Wow, git can be so confusing sometimes\nOne of the things I’ve really enjoyed chatting with Emi is our ability to ‘chat nerd’. I define Nerd Chats as conversations, where I get to talk passionately about a technical aspect that only so few would enjoy. I’ve found a special kind of connection with Emi through our ‘nerd chats’. It’s that rare space where I can passionately unravel the intricacies of a technical challenge, knowing she gets it. It’s the particular developer problems that only other developers (nerds) will be able to relate to and understand the frustration (or enjoyment depending on the chat).\nOur ‘nerd chat’ centered on a shared version control headache, specifically the dreaded “oh wait, I didn’t actually want to commit that!”. The solution: ‘Ctrl + Z’, the undo button, the digital form of an ‘UNO reverse card’. Fortunately, there’s a few git commands that offer exactly this.3 Unfortunately, these features were not working how we wanted them to.\n3 If you’ve got this problem too, I’d recommend looking at git revert, git commit --amend, git reset. Here’s a stackoverflow that might help.Familiar cycles through Google, Stackoverflow posts, and chatGPTing, a developer gets to a point of frustration, where best practices slowly start to fade. Instead of using these more appropriate, much better practice approaches, I confess and succumb to the lure of familiarity skipping the ‘correct’ approach. I push my ego and perfectionism aside and end up deleting my entire project folder and simply clone the remote repo again. Problem solved, because the problem never existed, a clean slate! Drastic solutions for seamingly simple problems. 😀\nWe exchange knowing nods mixed with ammusement and ’smh’ing; a couple of laughs, smiles, and moving on with our conversation."
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#setting-up-the-goal-posts",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#setting-up-the-goal-posts",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "Setting up the goal posts",
    "text": "Setting up the goal posts\nThe rest of my session with Emi flew by as I broadly described the goals I’m setting for our mentoring. We talked about everything – from coding challenges to big-picture career/travel/life aspirations. It was amazing to have someone really actively listen and help me clarify what I want to achieve.\n\n1. Development of an R Package(s)\nA primary goal is the creation of an R package(s). I want it to be relatively simple, but extremely well documented (both the package documentation, as well as the process of creating the package itself). What I’m aiming for isn’t some sprawling, all-encompassing tool, but something simple, focused, and extremely well-documented.\nBig emphasis on following Hadley and Jennifer’s best practices using their book: R Packages (2e). Most R packages I see are these massive, powerful things, and that’s amazing! But I want to show that even a small package can have high clarity and usability. I want it to be a learning tool for myself, and hopefully for others who are just getting started with package development.\nThankfully, I’m planning to blend this with my open-source goals. I want to document everything – the thought process, the challenges, the “aha!” moments. And this bleeds nicely into my 2nd mentoring goal.\n\n\n2. Contribution to Open Source Public Sector Data and Software\nThere are some incredible packages out there, like galah, readabs, and absmapsdata, that are doing really meaningful work. I’d love to contribute to these projects and learn from the amazing developers behind them. Also, interested in learning more about OPEN and FAIR data practices.\nAnd then there’s the runapp community. It’s full of so many talented and experienced users, and I’m eager to learn from their expertise. I want to participate more actively in their discussions, contribute to their projects, and build connections with these people.\nI think by blending my package development with open-source contributions and active participation in communities like runapp, I can really make a difference, but hopefully encourage others to engage with this goal. Plus, it’s a fantastic opportunity to learn from the best!\nFrom my experience in the public service, I’ve seen how easy it is to fall into the “this is how we’ve always done it” trap, or to shy away from new ideas due to perceived risks or scope limitations. I want to challenge that, and show that we can embrace modern data practices and use these open source tools more effectively.\n\n\n3. Sharing the journey with others (+ future Johann)\nI’ve already touched on this 3rd goal in the introduction. If you’ve made it this far, you know I’m a big believer in documenting and archiving ideas and conversations. I want these blog posts to be more than just updates; I want them to be a record of my learning process4, a place where I can reflect on what I’m discovering and share it with others.\n4 Teaching someone else is one of the best ways I learn!5 Communication is really the most important step in the data food chain.Ideally, these posts will help me practice my writing skills (even if it is informal writing). I want to get better at articulating my thoughts and experiences, especially when it comes to technical stuff.5\nMore importantly, I want to use this platform to inform and encourage others (especially future Johann reading this to keep being inspired and working on these home projects), particularly those in the public sector, to get involved with open source and data best practices. So many people in the public service use R and rely on these incredible packages, but they might not realise the effort behind them or the communities that build them. I want to shine a light on that, to show that anyone can contribute and learn.\n\n\n4. Exploration of Decision-making Parallels\nThis goal will be nice to balance all of the possible technical data chat that Emi will have to endure. The main challenges surrounding this goal is a broader challenge of being in your 20s. Some people in my cohort of 20-30 year olds are doing all sorts of fun things:\n\nSome are living it up in Sydney or Melbourne in their early professional careers.\nSome are taking the risk and moving to the other side of Australia to Perth or Darwin.\nOthers are packing up shop and moving overseas.\nA few others have fully packed up a backpack and travelling the world.\nI don’t know many, but some are buying homes and starting families.\nAnd I’m certain the majority have no idea what they’re doing themselves.\n\nThe choice paralysis challenge combines all sorts of avenues of life, like friends, families, relationships, career pathways, travel experiences, accessibility to services, structure & routine, adventure & excitement. It will be super valuable chatting to Emi and her perspectives. Her advice so far: “Talk to many different people and get a broad range of views”.\n\n\n5. Moving from mentee to mentor to teacher to coach\nI’ve always been drawn to mentoring and teaching. It’s a space where I feel truly inspired and alive6, where my professional and personal passions intersect. It’s a major driver for me, a real spark that makes work feel meaningful.\n6 Like truly alive, that feeling when you’re really just focusing and zoned in on the moment.So, naturally, one of my goals is to explore this further, and I’m particularly excited to discuss it with Emi, given her extensive teaching experience, especially in the university and research environment. Emi will have plenty of knowledge on balancing out the tedious task of teaching such as administration and middle/upper management as well as teaching coaching; the fun stuff, witnessing someone have the ‘aha!’ moment.\nI want to explore questions like:\n\n“How do you cultivate that supportive environment where people feel empowered to learn and grow?”\n“How have you used R as a teaching tool?”\n“How can I best use my passion for teaching in my role as a data professional?”"
  },
  {
    "objectID": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#the-end-thanks-for-reading",
    "href": "blog/ssa_mentoring_with_emi_2025_04_01/index.html#the-end-thanks-for-reading",
    "title": "SSA Mentoring with Emi (April 2025)",
    "section": "The end, thanks for reading! 😄",
    "text": "The end, thanks for reading! 😄\nI left feeling energised and like I have a real plan. Thanks Emi!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Johann Wagner",
    "section": "",
    "text": "Hi there, I’m Johann (he/him)! 👋\n\nNice to meet you! 😊\n\nI’m a Data Scientist working at Jobs and Skills Australia (Australian Government). I completed my Bachelor of Economics and Bachelor of Science specialising in Quantitative Environmental Modelling, Econometrics, and Applied Statistics at the Australian National University (2023). I love learning about all things data, spending quality time with friends, endurance sports (type 2 fun), and travelling. 📊📚💡\n\n\n\n🌱 My Interests:\n\nContinuous self-learning 📚\n\nData Science\nEconomics / Health Analytics\nOpen Science\n\nEndurance sports 🚲\n\nBikepacking\nAdventure Racing\nTrail Running\n\nData-driven solutions 📊\n\nEconomic Modelling\nEnvironmental Modelling\nSpatial Visualisation\n\n\n\n\n\n📈 My Broad Goals:\n\nEmbrace quality time spent with friends and family enjoying fun adventures 😋\nNavigate the vast data role ecosystem and try and figure out where I best fit in 💻\nImmerse myself in the data community by sharing my projects ✏️\nContribute to open source software and promote Open Science! 🧪\n\n\n\n\n🌱 My Current Experience:\n\nData Scientist at Jobs and Skills Australia (2024 - Present)\nEconomic / Data Analyst at the Department of Education (2023 - 2024) 🏛\nIntern Consultant at Nous Group (2022) 💼\nStudent Data Scientist at the Australian Bureau of Statistics (2022) 📊\nData Officer at ACT Health (2021 - 2022) 🦠\n\n\n\n\n✅ Moving Forward:\n\nAlways keen to chat about all things data, R programming, and open source software. I’m also a big fan of open science. 🌱🎓\nShoot me a message on LinkedIn or via email or any other method you prefer! 🤓\n\n\n\n\n🔗 Links:\nHere are some useful links you might be interested in:\n\n💼 CV\n📊 Data Projects\n✏️ Blog\n🤓 GitHub\n\n\n\n\n📙 Acknowledgements:\nThis website was built using Quarto.\nThe source code can be found on my GitHub.\nThanks to Marvin Schmitt, Melissa Van Bussell, Samantha Csik for their tutorials on how to build a website using Quarto. Their clarity of instructions made it easy to build this website. I encourage you to check out their work. Quarto also has some great documentation.\n\nThanks for reading until the end. 💅💅💅"
  }
]